TextBook:Complete
Directed Acyclic Graph

Data:
RetailData:

ReadingData From System:
val staticDataFrame = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/retail-data/by-day")

Creating a TempTable:
staticDataFrame.createOrReplaceTempView("retail_data")




Assigning Schema to a variable:
val staticSchema = staticDataFrame.schema

ImportingPacakges:
import org.apache.spark.sql.functions.{window, column, desc, col}

Sale hours During which a given customer makes a large purchase:
staticDataFrame.selectExpr("CustomerId","(UnitPrice * Quantity) as total_cost","InvoiceDate").groupBy(col("CustomerId"), window(col("InvoiceDate"), "1 day")).sum("total_cost").show(5)
	-- to know the amount the customer spent = (UnitPrice * Quantity)
	-- Grouping by CustomerID and where day is "1"					
	-- then we are summing the total cost

Setting Number of partitions:
spark.conf.set("spark.sql.shuffle.partitions", "5")

StructuredStreaming:	
val streamingDataFrame = spark.readStream.schema(staticSchema).option("maxFilesPerTrigger", 1).format("csv").option("header", "true").load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/retail-data/by-day")
	-- readStream is something for reading a file.
	-- maxFilesPerTrigger (default: no max limit): sets the maximum number of new files to be considered in every trigger. Sets the max number files to be triggered in a Kickoff or a trigger 1 mean 1 and 10 means 10.

streamingDataFrame.isStreaming
	-- we can check if our data is streaming and it returns true.

val purchaseByCustomerPerHour = streamingDataFrame.selectExpr("CustomerId","(UnitPrice * Quantity) as total_cost","InvoiceDate").groupBy($"CustomerId", window($"InvoiceDate", "1 day")).sum("total_cost")
	-- Here we want to know the amount spent by customer in a do so we are multiplying unitprice with quantity and naming it as total cost
	-- Grouping all the customer detils by a single day (we are using the window function and metioning for how manydays)
	-- then summating then summing the total_cost column


purchaseByCustomerPerHour.writeStream.format("memory").queryName("customer_purchases").outputMode("complete").start()
	-- Here we are writing the data we streamed and performed some data processing operations using writeStream options 
	-- after writeStream we need to understand where which should write i.e. in Memory(RAM) (or) MemorySink 
	-- We are also giving a name to the data or dataframe which we are writting as customer_purchases
	-- complete is all the counts should be in the table
	-- Start() - when we start the stream we can run the queries against the data to debug or to check how it looks like before writting it into production sink.
	-- Here we are writting to a memorysink and we don't wanna write and see on console we can do as below:
purchaseByCustomerPerHour.writeStream.format("console").queryName("customer_purchases_2").outputMode("complete").start()
	-- Anyways we are not gonns use any of this in production but we can do this before going to production for understanding.


spark.sql("""SELECT * FROM customer_purchases ORDER BY `sum(total_cost)` DESC""").show(5)

val bg = staticDataFrame.selectExpr("CustomerId","(UnitPrice * Quantity) as total_cost","InvoiceDate").groupBy(col("CustomerId"), window(col("InvoiceDate"), "1 day")).sum("total_cost").filter($"CustomerId" === "14126.0").show()
	-- FilterExample filtering with one CustomerID Syntax: filter(dataFrame(coloumn name) === "which coloumn element") Syntax:2: filter($"coloumn name") === "which coloumn element")
	-- We can represent a dataframe with a $ in some cases.

import org.apache.spark.sql.functions.date_format
	We should import this package to implement any functions related to the below videos

val preppedDataFrame = staticDataFrame.na.fill(0).withColumn("day_of_week", date_format($"InvoiceDate", "EEEE")).coalesce(5)		
	-- As we know we are having different types of data but MLlib will require only numeric values but the data which is in our dataframe including timeStamps, Integers, String.
	-- We will use DataFrame transformation and turn the DataFrame into numeric values
	-- As we know we are having null values in the DataSet (or) DataFrame and we are filling them with "0"s to sucessfully complete our transformations.
	-- We are creating a new column day_of_week by using data_format function (or) API and getting the day of the week like Monday.
	-- syntax of data_format("AnyTimeStampColoumn", what is the type of info you want from the time Stamp "yy","dd","EEEE") in the above case we are getting name of the week.
	-- coalesce(5) means number of thread which we wanna you form this transformation.

val trainDataFrame = preppedDataFrame.where("InvoiceDate < '2011-07-01'")
val testDataFrame = preppedDataFrame.where("InvoiceDate >= '2011-07-01'")
	-- Now we are having a DataFrame and we will categorise that into 2 types of data one is TrainDataFrame and which having a invoiceDate < 2011-07-01 and testDataFrame which is having invoice date >= to 2011-07-01

trainDataFrame.count()
testDataFrame.count()	
	-- We have splitted the data and checking the count on both the dataFramed.
	-- These above transformations are GeneralDataFrame operationa SparkMLlib provides some function (or) API's to reduce some of our steps and automates some of our general transformations.
	-- Below we will see somthing called as StringIndexer.

import org.apache.spark.ml.feature.StringIndexer
	-- We are puting some import statements
val indexer = new StringIndexer().setInputCol("day_of_week").setOutputCol("day_of_week_index")
	-- In the above statement we are converting the day_of_week (which is String) coloumn into corresponding numeric values(string to numbers).
	-- So may be Spark will take saturday as "6" and Monday as "1" so we are stating the the saturay is greater than monday which obviously didn't want
	-- To fix the above problem we need "OneHotEncoder"

display(indexer.fit(trainDataFrame).transform(trainDataFrame))

import org.apache.spark.ml.feature.OneHotEncoder
	-- Here we are importing another pacakage to use hotEncoder.

val encoder = new OneHotEncoder().setInputCol("day_of_week_index").setOutputCol("day_of_week_encoded")
	-- the boolean flag states weather the day of the week with relevant coloumn or not
	-- each of this will result in a set of coloums that we will assemble as (or) into a vector
	-- all machine learning algorithsm in spark takes input as only vectors and which ,must be numerical values

import org.apache.spark.ml.feature.VectorAssembler
	-- we are importing a package which is used before using a vector assembler

val vectorAssembler = new VectorAssembler().setInputCols(Array("UnitPrice", "Quantity", "day_of_week_encoded"))e.setOutputCol("features")	
	-- As we said we are using a vectorassemeble and giving 3 input coloumns and converting that or assemebling that into a vector.
	-- Here we are having 3 keys features i.e. unitprice,quantity,day_of_the_ week so we will set this up into a pipeline(schema) so what ever the data which come later will go through the exact same process.

import org.apache.spark.ml.Pipeline
	-- to use the pipelines we are importing a pacakge here

val transformationPipeline = new Pipeline().setStages(Array(indexer, encoder, vectorAssembler))
	-- Here we are setting a pipline we can use this for our future data

val fittedPipeline = transformationPipeline.fit(trainDataFrame)
	-- preparing for training is a 2 steps process. first we are feed our data set to the transformer 
	-- Our string indexer also should know howmany unique values there are to be indexed 
	-- After this encoding is easy but spark must look all the distinct values in the column to be indexed in the column to know later on..
	-- After we fit the traning data perfectly into that pipeline 
	-- Now we use our data in consistent format and repetable way

val transformedTraining = fittedPipeline.transform(trainDataFrame)
	-- We have fitted the trainDataFrame in the above step and then we will transform the train data frame 

	transformedTraining.cache()
	-- We are caching the data using cache
import org.apache.spark.ml.clustering.KMeans
	-- We are importing a pacakge to use Kmeans

val kmeans = new KMeans().setK(20).setSeed(1L)
	-- In spark training machine learning is 2 phase processs 
				1.We initialize a untrained model
				2.and then we train it
	-- There is always a 2 types of algorithms in MLLib DataFrame API
				1. They call as algorithms for untrained models
				2. They call as algorithmmodel for trained models
	-- In the above example we have create a Kmeans and then K-meansModel
	-- Here we created K-means	now lets fit the cached transformed traning into the K-means:
val kmModel = kmeans.fit(transformedTraining)
	-- Here we are fitting our transformed data into a K-means thing
kmModel.computeCost(transformedTraining)
	-- Here we can compute cost of the transformedData using K-means.
	-- Till above we have worked with trained DataFrame that is half our data but for the other half of the data i.e testDataFrame we don't have to do all that we can do as below
val transformedTest = fittedPipeline.transform(testDataFrame)
	-- Here we are transforming the testdata frame using the fittedpipeline which we have created using the trainedDataSet(algorithmmodel)
kmModel.computeCost(transformedTest)
	-- Here we can compute the cost directly for the transformed testdataframe which we stored in transformedTest and compute the cost :)
										
display(indexer.fit(trainDataFrame).transform(trainDataFrame))
	-- This the way to display a tarnsformed Data

-------------
val parra = spark.sparkContext.parallelize(Seq(1, 2, 3)).toDF()	
	-- Here we are creating data parallelizing the Data using paralleise in RDD's and then we are changing that into a DataFrame.
-------------

(sparkDF <- read.df("/Users/kartikeyan/Desktop/TextbookData.csv",source = "csv", header="true", inferSchema = "true"))
	-- Here we are reading the airportData using SparkR and creating an Dataframe where source is csv header true and inferschema is true.
sparkDF %>%orderBy(desc(sparkDF$count)) %>%groupBy("ORIGIN_COUNTRY_NAME") %>%count() %>%limit(10) %>%collect()
 
-------------

val df = spark.range(500).toDF("number")
 	-- here we are using range function taking 500 number and then converting into DF by saying toDF and also giving a name to a DF

df.select($"number" + 10)
	-- Here we are adding 10 to the number which are already there in the number DataFrame 
	-- Here this addition operation happenes because the spark converst the language which we have written internally into catalyst of same datatype in its own stayle
--------------
spark.range(2).toDF().collect()
	-- we are creating a row or record from scratch using range and converting that to DF and performing an action called as collect
--------------
import org.apache.spark.sql.types._
	-- importing a pacake of spark for types
val b = ByteType
	-- We are declaring a val b as bytetype
import org.apache.spark.sql.types.DataTypes;
	-- importing spark datatypes with this package
ByteType x = DataTypes.ByteType;
	-- Datatype of bytetype.
-- go to this loaction and then ./SparkR to run the "R"
----------------
val df = spark.read.format("json").load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/json")
	-- Here we are reading file and creating a dataframe out of it and th format we are reading should be mentions first and that would be jason
	-- After that we are using load API to literally load all the data which is present the path and the end directory.
df.printSchema()
	-- the above statement will show us the schema which is present in the Dataframe in this case it has picked the schema present the datasets jason files.
spark.read.format("json").load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json").schema
	-- Here we are loading the format and reading the schema by saying schema only not printSchema()
Output:
org.apache.spark.sql.types.StructType = ...
StructType(StructField(DEST_COUNTRY_NAME,StringType,true),
StructField(ORIGIN_COUNTRY_NAME,StringType,true),
StructField(count,LongType,true))
	-- Schema is StructType and made of number of fields called as StructFields
	-- There StructField contains(NameOfColoumn,DataTypeOfColoumn,BooleanType) like above.
	-- Boolean flag will specify if the coloumn contains missing or null values in them.
	-- By this we are getting the metadata of the coloumn to specify. and in the metadat is somthing which describes the coloumn in a data set.
	-- Schema can also contains complex StructType and if the Schema which mentioned doesn't match the data Spark will throw an error.

import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
import org.apache.spark.sql.types.Metadata	 
	-- Importing some packages to define the schema.
val myManualSchema = StructType(Array(StructField("DEST_COUNTRY_NAME", StringType, true),StructField("ORIGIN_COUNTRY_NAME", StringType, true),StructField("count", LongType, false,Metadata.fromJson("{\"hello\":\"world\"}"))))
	-- Here we are defining a schema manually for a dataframe 
val df = spark.read.format("json").schema(myManualSchema).load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json")
	-- Assigning the schema to the dataframe and then loading the data using the manual schema like above we need say .schema(SchemaName)	
import org.apache.spark.sql.functions.{col, column}
	-- We are importing the packages for coloumn function
col("someColumnName")
column("someColumnName")	
	-- here we are using both coloumn functions and referring to a column.
$"myColumn" (or) 'myColumn	
	-- As we all know scala is something very beuatifull which will allow us to use or refer to the column name as above with $"columnName" 'ColoumnName
df.col("count")
	-- We are referring to coloumn in the DataFrame.
(((col("someCol") + 5) * 200) - 6) < col("otherCol")
	-- here we are using col to perform some operations in the dataframe
import org.apache.spark.sql.functions.expr
	-- We are importing packages to us Expr function
expr("(((someCol + 5) * 200) - 6) < otherCol")
	-- Here we are using the Expr function performa and operation on the same DF which is a DataFrame.
spark.read.format("json").load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json").columns
	-- although we can see all the coloumns if we still want to refer all the coloumns in a code we can simpy use DataFrame.columns.
df.first()
	-- this will give first row object or a first record of a DataFrame.
import org.apache.spark.sql.Row
	-- We are importing a pacake to import rows in a DataFrame 
val myRow = Row("Hello", null, 1, false)
	-- We are creatina row and assigning it to a value myRow
myRow(0) // type Any
myRow(0).asInstanceOf[String] // String
myRow.getString(0) // String
myRow.getInt(2) // Int
	-- this how we acess rows in scala
--------------
val df = spark.read.format("json").load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json")
	-- Here we are specifiying the format of the file and just loading the data from the file and converting into an DataFrame.
df.createOrReplaceTempView("dfTable")
	-- We are creating a temp table so that we acess and manipulate the using SQL views and tables
	-- We can also create a DataFrame on fly by taking set of rows and converting then into DataFrames.
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
	-- We are importing some packages to use SQL for queriying.
val myManualSchema = new StructType(Array(new StructField("some", StringType, true),new StructField("col", StringType, true),new StructField("names", LongType, false)))	 				
	-- Here we are just creating a schema manually and in schema we are defining the coloumn name and datatype which we are using
val myRows = Seq(Row("Hello", null, 1L))
	-- Here we are declaring or creating a row.
val myRDD = spark.sparkContext.parallelize(myRows)
	-- with the help of RDD's we are parllelising the data in the rows.
val myDf = spark.createDataFrame(myRDD, myManualSchema)
	-- We are creating a DataFrame simply adding the manual schema and row we have declared an converting them into a DataFrame using createDataFrame API
myDf.show()
	-- Will show us a Dataframe with the coloumn names which we have mentioned and then row which we have created in tabulat format like and DataFrame.
val myDF = Seq(("Hello", 2, 1L)).toDF("col1", "col2", "col3")
	-- We can also simply do it like below but we shouldn't do thos because this is not suggestable in production.
SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2
	-- In SQL we are gonna comman like this if we want to acess a coloumns 	DEST_COUNTRY_NAME from table dfTable and the we wann see 2 records.
df.select("DEST_COUNTRY_NAME").show(2)
	-- Writting the same with the select command in spark. Dataframe.Select a coloumn .show(2) reocrds.
df.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2)
	-- We can also select multiple coloumns using the same 
import org.apache.spark.sql.functions.{expr, col, column}
 	-- We are importing a package to use the below thinsg	
df.select("DEST_COUNTRY_NAME")
	-- We can just acess the coloumn by using the select command
df.select(col("DEST_COUNTRY_NAME"))
	-- By just saying col infront of column name
df.select(column("DEST_COUNTRY_NAME"))
	-- By using the entire coloums declartion
df.select('DEST_COUNTRY_NAME)
	-- By using just ' in the starting of the coloumn name
df.select($"DEST_COUNTRY_NAME")
	-- By using a dollar
df.select(expr("DEST_COUNTRY_NAME"))
	-- by using an expr.
df.select(df.col("DEST_COUNTRY_NAME"),col("DEST_COUNTRY_NAME"),column("DEST_COUNTRY_NAME"),'DEST_COUNTRY_NAME,$"DEST_COUNTRY_NAME",expr("DEST_COUNTRY_NAME")).show(2)								 	
	-- Everything of above and all the declaration for acesssing the coloumns will result in the same coloumn we specified.
df.select(col("DEST_COUNTRY_NAME"), "DEST_COUNTRY_NAME")
	-- We cannot do this because it is gonna throw and complie time error.
df.select(expr("DEST_COUNTRY_NAME AS destination")).show(2)
	-- Here we are using select and expr to rename the country and we are renaming ot by using as keyword.
	SELECT DEST_COUNTRY_NAME as destination FROM dfTable LIMIT 2
df.select(expr("DEST_COUNTRY_NAME as destination").alias("DEST_COUNTRY_NAME")).show(2)			
	-- here we are expr and changing the country name by using an as keyword and then changigng it back to the orignal name by using alias
df.selectExpr("DEST_COUNTRY_NAME as newColumnName", "DEST_COUNTRY_NAME").show(2)
	-- so here for multie purpose spark help us combine select and Expr and we can do wonders with this
df.selectExpr("*", // include all original columns"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry").show(2)
	-- so here we are selectExpr as we are doing multiple operations together. to select everything from the complete table we have used a "*"
	-- After that we have used a condition to see whether the DEST_COUNTRY_NAME is equal ORIGIN_COUNTRY_NAME and this gonna return true and false and then we are naming this entire thing as and coloumns withinCountry
df.createOrReplaceTempView("dfTable")
	-- We are creating and temptable to acess the dataframe using SQL commands.
spark.sql("SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry FROM dfTable")
	-- Here we are checking if DEST_COUNTRY_NAME is equal to the ORIGIN_COUNTRY_NAME an it will give true or false and we are saving that information on to the different coloumns name  withinCountry
df.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show(2)
	-- Here we are getting Average of count and the then count of DEST_COUNTRY_NAME but which should be disctinct.
spark.sql("SELECT avg(count), count(distinct(DEST_COUNTRY_NAME)) FROM dfTable LIMIT 2")		
	-- Here we are doing the same that we did using the DataFrames we have taken the avg of the count coloumns and then we are counting the countired in DEST_COUNTRY_NAME and we are only counting distinct things or countries.
df.withColumn("numberOne", lit(1)).show(2)
	-- Here we are creating another call with a name numberOne and then we will assign 1 to all coloumns 
df.withColumn("withinCountry", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME")).show(2)
	-- Here we are using withcoloumn function and then assigning it boolean flags for checking if the country name in the above both columns are equal
	-- Here the with coloumn function takes 2 parametere 
					1.Coloumn Name 
					2.Expr that created a value for the given row in a DataFrame.
df.withColumn("Destination", expr("DEST_COUNTRY_NAME")).columns
	-- here we can also rename the coloumn using the with coloumn function and Expr and we that in above example
df.withColumnRenamed("DEST_COUNTRY_NAME", "dest").columns
	-- This an another way of renaming a coloumn and  here we are using a function which is called as withColumnRenamed and this will take 2 parameters and that would be Original coloumn name and the name of the coloumn which you wannaa change into
val dfWithLongColName = df.withColumn("This Long Column-Name",expr("ORIGIN_COUNTRY_NAME"))									
	-- Here we are creating a coloumn name with space and all we can remove them using a backtick like below.
dfWithLongColName.selectExpr("`This Long Column-Name`","`This Long Column-Name` as `new col`").show(2)
	-- here we are using back ticks with a coloumn and we can avoid sapces which is easy and we are also chnaging the name of the column.
dfWithLongColName.select(col("This Long Column-Name")).columns
	-- here also we are doing the same thing 
set spark.sql.caseSensitive true
	-- Defaultly spark is case insensitive but if we wanna change that we need to set the above thing to true.
dfWithLongColName.select(col("This Long Column-Name")).columns
df.drop("ORIGIN_COUNTRY_NAME").columns
	-- If we want to delete a coloumn then we can use this drop function 
dfWithLongColName.drop("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME")
	-- We can drop multiple columns at a time by passing multiple column names.
df.withColumn("count2", col("count").cast("long"))
	-- We can change a column type by casting and that can be don by cast
	-- Here we are creating a column and then we are filling it with the cast of the column count.
spark.sql("SELECT *, cast(count as long) AS count2 FROM dfTable")
	-- we are doing the same in the SQL format.
df.filter('count < 2).show(2)
	-- here we are using a filter operation on column count to find out thw rows which are having the count value <2
SELECT * FROM dfTable WHERE count < 2 LIMIT 2
	-- We are doing the same in SQL
df.filter(('count < 2)&&('DEST_COUNTRY_NAME =!= "Croatia")).show()
	-- If wanna use filter with multipl columns then we gotta use overloaded filter with a "&&" between the 2 columns
df.filter(('count < 2)&&('DEST_COUNTRY_NAME === "Kosovo")).show()
	-- while using == we use 3 === instead of 2
df.where('count<2).where(('ORIGIN_COUNTRY_NAME) =!= "Croatia").show(2)
	-- using where instead of filter and doing sperately on multiple columns like above
spark.sql(SELECT * FROM dfTable WHERE count < 2 AND ORIGIN_COUNTRY_NAME != "Croatia"LIMIT 2)
	-- we are using SQL for performing the same operation we did with where and filter.
df.select('DEST_COUNTRY_NAME,'ORIGIN_COUNTRY_NAME).distinct().count()
	-- Here we are using distinct to take out the duplicates from the above columns
df.select("ORIGIN_COUNTRY_NAME").distinct().count()
	-- Here we are doing distinct on the single columns.
spark.sql("SELECT COUNT(DISTINCT ORIGIN_COUNTRY_NAME) FROM dfTable")
	-- SQL way of using Distinct on a dataframe
val seed = 5
val withReplacement = false
val fraction = 0.5
df.sample(withReplacement, fraction, seed).count()
	-- This is somthing called sampling 
	-- 	here with replacement mean the replacemnet between the element allowed suppose we have 12,13 then (12,12),(12,13),(13,13) is allowed with out replacement means (12,13) is possible the both values cannot be same
	-- fraction mean if we have 100 records or rows then we want to sample 10 out of hundered we need to mention it as 0.10 and if we want 5 then we should say 0.5 the percentage of out of 100 is what we are considering here.
val dataFrames = df.randomSplit(Array(0.25, 0.75), seed)
	-- here we are splitting the entire data frame into 2 parts and out of that one part is 0.25 and another part is 0.75 and the seed will be seed.
import org.apache.spark.sql.Row:
	-- here we are importing a package to add rows to a DataFrame
val schema = df.schema
	-- This means that we are first taking the schema of the dataframe 
	-- When we say df.schema it is gonna be like this org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))
	-- So in the above statement we are assigning the schema of DataFrame df to a variable saying val schema = df.schema()
val newRows = Seq(Row("New Country", "Other Country", 5L),Row("New Country 2", "Other Country 3", 1L))	
	-- Here we are assigning creating a row like normally we would and giving each coloumn of record with and value 
	-- and after creating the rows we are assigning the same thing to a variable name newRows
val parallelizedRows = spark.sparkContext.parallelize(newRows)
	-- so as we know that we will not be able assign the rows directly the dataframe we are parallelising them.
	-- in this case we are parallelising the variable to which we have assigned the rows to.
val newDF = spark.createDataFrame(parallelizedRows, schema)
	-- Here we are creating a nre DataFrame with the rows which have created and the schema which we have generated and creating a new DataFrame with rows and schema.
df.union(newDF)
	-- here we are performing a uniion operation on the old df which we have on the newDF which we have by taking the union function.
df.union(newDF).where("count = 1").where($"ORIGIN_COUNTRY_NAME" =!= "United States").show() 	
	-- here we are performing a union operation in the first step.
	-- and then to check the column which we have added are there we are gonn do where($"ORIGIN_COUNTRY_NAME" =!= "United States").show().
	-- this will show the rows which we have added.
df.sort("count").show(5)
	-- here we are sorting the dataframe's column count
	-- and here we are not specifying any kind of ordering so it will be ordered defaultly in ascending order.
df.sort('count,'DEST_COUNTRY_NAME).show()
	-- here we are sorting the 2 column of a dataframe using the sort and even in this we have not mentioned any kind of sort so it will do in natuarl ascending order
df.orderBy('count, 'DEST_COUNTRY_NAME).show(5)	
	-- here we are using orderBy function instead of sort and ordering the count and DEST_COUNTRY_NAME column in the dataframe
import org.apache.spark.sql.functions.{desc, asc}
	-- here we are importing a package to use desc and asc for sorting the elements.
df.sort('count asc).show()
	-- here we are sorting the count column in asc that is in ascending order.
df.sort('count desc).show()
	-- here we are sorting the count column in desc order.
df.orderBy(desc('count), 'DEST_COUNTRY_NAME).show()
	-- here we are using orderBy and on count and DEST_COUNTRY_NAME
spark.sql("SELECT * FROM dfTable ORDER BY count DESC, DEST_COUNTRY_NAME ASC LIMIT 2)	 						
	-- SQL way of doing the same thing as the above.
asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last	
	-- we can use all this to specify to where we use nulls 
df.sortWithinPartitions('count)	
	-- This is used when we only want to sort within a partition.
df.limit(5).show()
	-- this will only take 5 rows from a partition
spark.sql("SELECT * FROM dfTable LIMIT 6")
	-- SQL way of representing the same.
df.rdd.getNumPartitions
	-- We are checking the number of partition in the df. or something like that. the result would be 1
df.repartition(5)
	-- when we do this it is turned into 5 and when we check we will get 5.
df.repartition(col("DEST_COUNTRY_NAME"))
	-- If we are gonna use the column like so many times it easy or useful to partition on the column like above.
df.repartition(5, col("DEST_COUNTRY_NAME"))
	-- we can also specify the column and also the number of the partition which we want to do on that column.
df.repartition(5, col("DEST_COUNTRY_NAME")).coalesce(2)
	-- here we are using coalesce to combine the partition which we have did.
val collectDF = df.limit(10)
	-- here we are taking the 10 rows of the dataframe and then assigning them to a variable.
collectDF.show() 
	-- then we are performing the show on the collectDF
collectDF.show(5, false)
	-- same
collectDF.collect()
	-- this will collect all the rows if there are many and will show only limited on the console.
toLocalIterator
	-- This is something will bring each partition into the driver and perform the operation if we have 5 partition then it is gonna bring 1 after 1 after 1 into the driver.
collectDF.toLocalIterator()
	-- this the we use the above 
	-- any collect of data is an expensive operation and if we are using any of the operation like collect on a dataset which is huge the node will crash 
	-- even to LocalIterator is also expensive and if the partition is big enough this is also gonna crash our node.
-------------------------------
val df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2010-12-01.csv")
	-- we are loading a file to create a DataFrame.
df.printSchema()
	-- will print schema of the DataFrame which we have created.
df.createOrReplaceTempView("dfTable")
	-- Creating a temp table to acess it with a SQL command.
import org.apache.spark.sql.functions.lit
	-- here we are importing a pacakage to use lit function.
df.select(lit(5), lit("five"), lit(5.0))
	-- here we are using select statement and in which we are selecting the lit function and then we are giving 3 different values and see how it converts into spark datatypes.
	-- lit(5) = IntegerType
	-- lit("five") = StringType
	-- lit(5.0) = DoubleType
import org.apache.spark.sql.functions.col
	-- here are importing some packages to work with booleans.

df.where(col("InvoiceNo").equalTo(536365)).select("InvoiceNo", "Description").show(5, false)
	-- Here we are using where to choose a coloumn and then we are filtering the invoice coloumn by the invoice number 536365
	-- then we are selecting the 2 coloumns which are to be shown in this case.
===(or)==
	--Scala has some particular semantics regarding the use of == and ===. In Spark, if you want to filter by equality you should
	use === (equal) or =!= (not equal). You can also use the not function and the equalTo method.	
import org.apache.spark.sql.functions.col
	-- Here we are importing a package
df.where(col("InvoiceNo") === 536365).select("InvoiceNo", "Description").show(5, false)																		-- here we are doing the same but here we arw using === instead on equal to.
df.where("InvoiceNo = 536365").show(5, false)		
	-- here this shows the data we need is simple way.
val priceFilter = col("UnitPrice") > 600
	-- here we are selecting the column elements of UnitPrice which are greater than 600 and assigning them to the variable
val descripFilter = col("Description").contains("POSTAGE")		
	-- Here we are selecting or filterning the column description that only contains postage and eliminating remaining all the elements.
df.where('StockCode.isin("DOT")).where(priceFilter.or(descripFilter)).show()	
	-- here we are only taking only elements of column in stockcode which is DOT and where pricefilter (or) desccipfilter
saprk.sql("SELECT * FROM dfTable WHERE StockCode in ("DOT") AND(UnitPrice > 600 OR instr(Description, "POSTAGE") >= 1)	
	-- SQL way if doing this
val DOTCodeFilter = col("StockCode") === "DOT"(this only have 2 values)
	-- Here we are only seperating the value of coloumns which have only DOT in them and then assigning it to a varaiable.
val priceFilter = col("UnitPrice") > 600 (this only have one value)
	-- We are also filtering the unitprice colum values which are >600 and then assigning them to a coloumn
val descripFilter = col("Description").contains("POSTAGE")(this have 5 values )
	-- As we have string with words in this column Description and we wanna filter out the column with value which has postage and assigning it to a varaible so to check in an entire string we need to use contains instead of an opearator.
df.withColumn("isExpensive", DOTCodeFilter.and(priceFilter.or(descripFilter))).where("isExpensive").select("unitPrice", "isExpensive").show(5)
	-- here we are combining evrything and it is a little complicated according to the understanding.
SELECT UnitPrice, (StockCode = 'DOT' AND
spark.sql(UnitPrice > 600 OR instr(Description, "POSTAGE") >= 1)) as isExpensive FROM dfTableWHERE (StockCode = 'DOT' AND(UnitPrice > 600 OR instr(Description, "POSTAGE") >= 1))	
	-- SQL version of same things
df.where(col("Description").eqNullSafe("hello")).show()
	-- to perform the same even if we have nulls shoyld use this.
Ex:Suppose we have missrecorded the item in out Retail DataSet basically the dataset should contain the quantity of the (the current quantity * the unit price) + 5
	-- this is out first numberic function and also the first time we are using pow function.
import org.apache.spark.sql.functions.{expr, pow}
	-- We are importing a package here to use pow and expr.		
val fabricatedQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5
	-- Here we are just defining what want into so kind of schema and you know assigning to a varaiable and we will use that later in the expression
df.select(expr("CustomerId"), fabricatedQuantity.alias("realQuantity")).show(2)	
	-- here by using the select function we are acessing the column and as we are working with the equation which is stored in fabricated quantity we need to use Expr 
df.selectExpr("CustomerId","(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity").show(2)	
	-- Here we are using select +expr together and we are not assigning the equation to any variable but we are doing it in the select Expr itself.
spark.sql(SELECT customerId, (POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantityFROM dfTable)
	-- SQL way of representing the same things.
df.select(round(col("UnitPrice"), 1).alias("rounded"), col("UnitPrice")).show(5)
	-- SomeTimes we will need round our values is we can use round function for that
df.select(round(lit("2.5")), bround(lit("2.5"))).show(2)
	-- Bround will exactly round it to a single number value.
spark.sql("SELECT round(2.5), bround(2.5)")
	-- SQL way of representing the same
df.describe().show()
	-- this will show count mean min max and standard deviation of all the columns
+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+
|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|
+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+
|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|
|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|
| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|
|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|
|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|
+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+
import org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max}
	-- we are importing this package so that we can these functions individually and precisely.
val colName = "UnitPrice"
	-- We are taking the untiprice coloums and colName 
val quantileProbs = Array(0.5)
	-- quantile probs as 0.5
val relError = 0.05
	-- relError as 0.05
df.stat.approxQuantile("UnitPrice", quantileProbs, relError)
	-- BY using the approxQuantile function we will be able to achive the approximate quantile of the column we have mentioned.
df.stat.crosstab("StockCode", "Quantity").show()
	-- Here we are using crosstab to perform crosstab on two columns.
df.stat.freqItems(Seq("StockCode", "Quantity")).show()		
	-- Here we are getting the frequent items from both the columns.
import org.apache.spark.sql.functions.monotonically_increasing_id
	-- Here we are importing a package for implemeneting a function.
df.select(monotonically_increasing_id()).show(2)
	-- Here we can assign a new ID to each row of the data frame using the 	monotonically_increasing_id
import org.apache.spark.sql.functions.{initcap}
	-- Here we are importing a package to use intCap
df.select(initcap(col("Description"))).show(2, false)	
	-- here we have used IntCap.
	-- This function will capitalize the every word in the given string which is seperated with a space from another word.
spark.sql(SELECT initcap(Description) FROM dfTable)
	-- SQL way of defining the same 
import org.apache.spark.sql.functions.{lower, upper}
	-- We are importing a package to use those strings.
df.select(col("Description"),lower(col("Description")),upper(lower(col("Description")))).show(2)					
	-- Here we are performing the upper case and lower case functions on the description column.
import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}
	-- We are importing this functions by this statmeent to implement thosde functions in the string
df.select(ltrim(lit(" HELLO ")).as("ltrim"),rtrim(lit(" HELLO ")).as("rtrim"),trim(lit(" HELLO ")).as("trim"),lpad(lit("HELLO"), 3, " ").as("lp"),rpad(lit("HELLO"), 10, " ").as("rp")).show(2)	
	-- If there are any spaces around the string we will be able to use lpad,ltrim, rpad and rtrim, trim to trim the spaces accordinly
RegularExpression:
	-- in order to use this regular expression we need use two more function and those would be regexp_extract and regexp_replace
	-- The above both functions extract and replace values as mentioned.
import org.apache.spark.sql.functions.regexp_replace
	-- Here we are doing a import statement in order to replace something in a string
val simpleColors = Seq("black", "white", "red", "green", "blue")
	-- Here we are creating a varaible with number of strings in it and we will use it for something else
val regexString = simpleColors.map(_.toUpperCase).mkString("|")
	-- Here we are converting everything the variable to upper case and then we are seperating each of them using a | 
df.select(regexp_replace(col("Description"), regexString, "COLOR").alias("color_clean"),col("Description")).show(2)			
	-- Here we are using regexp_replace and then we are giving the column name on which we are doing the operation on and then we are giving a string which has all the colours and then we are replacing everything in the coloumn which have colours which are mentioned in the string will be replaced with colour.
import org.apache.spark.sql.functions.translate
	-- Here we are importing a package to use a function called as translate 	
df.select(translate(col("Description"), "LEET", "1337"), col("Description")).show(2)
	-- So in this translate we can replace the values this is done at character leve or we can say it is done at letter level.
	-- We will replace all the instance of the character with the index of the replacement string which means every letter which is in replacement string will be changed all over the coloumn.
df.select(translate(col("Description"), "LEET", "1337"), col("Description")).show(2)
	-- Herw we are using DataFrame which is df and then from that we are gonna translate the letters which are in the column Description and we are gonna replace ann LEET with 1337 i.e l is replaced with 1 and E and E are replaced with 3 and then T is replaced with 7 
	-- the out can be seen below:
+----------------------------------+--------------------+
|translate(Description, LEET, 1337)|         Description|
+----------------------------------+--------------------+
|              WHI73 HANGING H3A...|WHITE HANGING HEA...|
|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|
+----------------------------------+--------------------+	
	-- so here we are taking select and in that we are selecting the column with the changes and also the orginal column.
spark.sql("SELECT translate(Description, 'LEET', '1337'), Description FROM dfTable")
	-- We are doing the same thing using SQL
We can also do many such thing like pulling out the first mentioned colour in th column like below
import org.apache.spark.sql.functions.regexp_extract	
	-- Here we are taking another function which is under regular expression.
val simpleColors = Seq("black", "white", "red", "green", "blue")
	-- Here we are creating seq of colur and then assigning them to a variable 
val regexString = simpleColors.map(_.toUpperCase).mkString("(", "|", ")")
	-- then we are using a map function then changing them to UpperCase and then we are making the string of , into a string of |
df.select(regexp_extract(col("Description"), regexString, 1).alias("color_clean"),col("Description")).show(2)					
	-- Here we are using regexp_extract to extract the value from the string and this is by select operation and the the extraxtion is don on the column description and we are giving the string with the colurs and then a "1" and naming this entire column as color clean and then taking the normal column
SELECT regexp_extract(Description, '(BLACK|WHITE|RED|GREEN|BLUE)', 1),DescriptionFROM dfTable	
	-- SQL way of representing the same 
val containsBlack = col("Description").contains("BLACK")
	-- We are checking th column description if it has black and then assigning to the varaible
val containsWhite = col("DESCRIPTION").contains("WHITE")
	-- We are checking for whitw and then assigning 
df.withColumn("hasSimpleColor", containsBlack.or(containsWhite)).where("hasSimpleColor").select("Description").show(3, false)			
	-- here we are using withColumn and then naming a column which doesn't exist and then assigning it with the contains black or white 
	-- Where("hasSimpleColor") will directly pick ony the row with values which have white or black in them.
SELECT Description FROM dfTable WHERE instr(Description, 'BLACK') >= 1 OR instr(Description, 'WHITE') >= 1 	
	-- SQL way of representing the same.
-------------------------------------------------------------------------------------------------------------------------------------	
val simpleColors = Seq("black", "white", "red", "green", "blue")																	|
val selectedColumns = simpleColors.map(color => {col("Description").contains(color.toUpperCase).alias(s"is_$color")}):+expr("*")	|
df.select(selectedColumns:_*).where(col("is_white").or(col("is_red"))).select("Description").show(3, false)							|
-------------------------------------------------------------------------------------------------------------------------------------
df.printSchema()
 |-- InvoiceNo: string (nullable = true)
 |-- StockCode: string (nullable = true)
 |-- Description: string (nullable = true)
 |-- Quantity: integer (nullable = true)
 |-- InvoiceDate: timestamp (nullable = true)
 |-- UnitPrice: double (nullable = true)
 |-- CustomerID: double (nullable = true)
 |-- Country: string (nullable = true)
 	-- In this data set it defaultly tool InvoiceData as TimeStamp.
import org.apache.spark.sql.functions.{current_date, current_timestamp}
	-- Here we are importing a package to implement the above function mentioned.
val dateDF = spark.range(10).withColumn("today", current_date()).withColumn("now", current_timestamp())	
 	-- Here we are creating a data frame with range 10 and we are creating column with name today and another with now and then we are assigning then with values of current_date and current_timestamp
dateDF.createOrReplaceTempView("dateTable")
 	-- Create a temptable.	
dateDF.printSchema()
  	--  |-- id: long (nullable = false)
 		|-- today: date (nullable = false)
 		|-- now: timestamp (nullable = false)	
import org.apache.spark.sql.functions.{date_add, date_sub}
	-- We are omporting this packages to implement those function 
dateDF.select(date_sub(col("today"), 5), date_add(col("today"), 5)).show(1)
	-- Here we are adding 5 days to today and subtracting 5 days from today. for that we are using date_sub and date_add
SELECT date_sub(today, 5), date_add(today, 5) FROM dateTable	
	-- SQL way of representing the same.
import org.apache.spark.sql.functions.{datediff, months_between, to_date}
	-- Here we are importing a package to implement those functions.
dateDF.withColumn("week_ago", date_sub(col("today"), 7)).select(datediff(col("week_ago"), col("today"))).show(1)
	-- Here first we are creating column and then assigning the column with the subtract date from to to 7 days and then selecting the weekago columns and the col today and we are checking the date difference.
dateDF.select(to_date(lit("2016-01-01")).alias("start"),to_date(lit("2017-05-22")).alias("end")).select(months_between(col("start"), col("end"))).show(1)
	-- Here we are using select and the giving the todate to another data and then nameing them as start and end and then we are using Months_between and checking all the mmonths between start and end.,
SELECT to_date('2016-01-01'), months_between('2016-01-01', '2017-01-01'),datediff('2016-01-01', '2017-01-01')FROM dateTable	
	-- SQL way of representing the same.
	-- The to_date function allows you toconvert a string to a date, optionally with a specified format	
import org.apache.spark.sql.functions.{to_date, lit}
	-- Here we are implementing a package with above given functions
spark.range(5).withColumn("date", lit("2017-01-01")).select(to_date(col("date"))).show(1)
	-- Spark will not throw erroe if cannot parse the data it will just give null.
dateDF.select(to_date(lit("2016-20-12")),to_date(lit("2017-12-11"))).show(1)
	-- here we are converting year-month-day to year-day-month spark will fail to convert but this is not gonna throw any error but simply it returns null
	-- We find this very difficult and tricky situations for bugs because some dates can match the correct format and some date may not. Spark doesn't know what is going on because its alls mixed up and spark will not be able to identify.
	-- So to avoid such kind of issues lets come up with a plan.
	-- First stemp we will specify the dates as JavaDateFormat.
import org.apache.spark.sql.functions.to_date		
	-- Here we are importing a fuction to import to_date function.
val dateFormat = "yyyy-dd-MM"	
	-- Here we are assigning a date format to a varaible.
val cleanDateDF = spark.range(1).select(to_date(lit("2017-12-11"), dateFormat).alias("date"),to_date(lit("2017-20-12"), dateFormat).alias("date2"))	
	-- We have declared a varaible with name cleanDateDF then we are using range function to assign the number and we are taking range(1) and then we are using select function in that we are using to_date function and after that then we are using lit to make spark understand this and we are are assigning the format as the variable dateformat which we have created and then renaming the table as date and then another one as another column.
SELECT to_date(date, 'yyyy-dd-MM'), to_date(date2, 'yyyy-dd-MM'), to_date(date) FROM dateTable2	
	-- SQL way of doing it
cleanDateDF.select(to_timestamp(col("date"), dateFormat)).show()
	-- We are doing the same but just that we are using to_timestamp instead of to_date
SELECT cast(to_date("2017-01-01", "yyyy-dd-MM") as timestamp)
	-- SQL way of doing the same.
cleanDateDF.filter(col("date2") > lit("2017-12-12")).show()
	-- after we get the timestamp and date in correct format comparing them is very easy.
	-- We can either compare date or timestamp or between both of them but according to we have to set the string ing right format like yyyy-mm-dd
import org.apache.spark.sql.functions.coalesce
	-- Here we are importing a package to use colaesce
df.select(coalesce(col("Description"), col("CustomerId"))).show()
	-- here we are using this to only give non null values first as we are not having nulls its gonna return normally.
	-- But the coalesce is used mainly to return the non null value first not the nulls.
ifnull, nullIf, nvl, and nvl2
	-- these are the method in SQL to avoid null values 
	-- We use all this in select expression on a DataFrame.
SELECTifnull(null, 'return_value'),nullif('value', 'value'),nvl(null, 'return_value'),nvl2('not_null', 'return_value', "else_value")FROM dfTable LIMIT 1	-- SQL way of representing all the above function and each of then have different meaning which need to be understood.
df.na.drop()
	-- This will drop all the null values from a column 
df.na.drop("any")
	-- the same
SELECT * FROM dfTable WHERE Description IS NOT NULL
	-- 	SQL way of representing the same.
	-- By specifying the word "any" means that if the column has any null values it is gonna drop the entire column.
	-- If you use "all" in case it will only drop the column if all values of the every column in a row is null. 
df.na.drop("all", Seq("StockCode", "InvoiceNo"))
	-- We cam also apply all for certain sets of columns and only drop it if all of them have nulls.
df.na.fill("All Null values become this string")
	-- By using we can fill all the null values with the above string.
	-- And in the above Example we are filling up the null values which are strings.
df.na.fill(5:Integer)	
	-- We can the do the same filling for integers.
df.na.fill(5:Double).
	-- We can use this to fill all the null in the column which is double type
val fillColValues = Map("StockCode" -> 5, "Description" -> "No Value")
	-- We can do the same filling by using map in scala that would be scala Map
	-- in the above statement we have defined the mapScala in the way we want and assigned it to the varaiable and we are gonna use that with DataFrame.
df.na.fill(fillColValues)
	-- here we are using fill and fill (variable which we have declared)												
df.na.replace("Description", Map("" -> "UNKNOWN"))
	-- We can use the replace this way to replace the null values.
df.selectExpr("(Description, InvoiceNo) as complex", "*")
	-- we are merging 2columns as complex 
val san = df.selectExpr("struct(Description, InvoiceNo) as complex", "*")
san.printSchema()
	-- we are merging them to struct type.
	|-- complex: struct (nullable = false)
 |    |-- Description: string (nullable = true)
 |    |-- InvoiceNo: string (nullable = true)
 |-- InvoiceNo: string (nullable = true)
 |-- StockCode: string (nullable = true)
 |-- Description: string (nullable = true)
 |-- Quantity: integer (nullable = true)
 |-- InvoiceDate: timestamp (nullable = true)
 |-- UnitPrice: double (nullable = true)
 |-- CustomerID: double (nullable = true)
 |-- Country: string (nullable = true)		
import org.apache.spark.sql.functions.struct
	-- we are importing a package to use struct type.
val complexDF = df.select(struct("Description", "InvoiceNo").alias("complex"))
	-- We are creating a complex struct type
complexDF.select("complex.Description")
	-- here we are making a complex type.
complexDF.select(col("complex").getField("Description"))
	-- and also getting the field description.
SELECT complex.* FROM complexDF
	-- SQL way of doing the same.
Arrays:
	-- To define arrays, let’s work through a use case. With our current data, our objective is to take everysingle word in our Description column and convert that into a row in our DataFrame.			
	-- The first task is to turn our Description column into a complex type, an array
	-- to convert this into a complex data we need to first split the description column like below.
import org.apache.spark.sql.functions.split
	-- For that we are importing a package related to a function of split
df.select(split(col("Description"), " ")).show(2)
	-- Here we are splitting the column Description by delimeter ","
SELECT split(Description, ' ') FROM dfTable
	-- SQL way of representing the same as above.
	-- This so powerful because this spark allows us to manipulate complex this complex type as another column.
df.select(split(col("Description"), " ").alias("array_col")).selectExpr("array_col[0]").show(2)				
	-- Here first we are selecting the column and splitting the column description using the delimeter and we are renaming the same as array_col and then we are selecting the first element from the array column [0].
import org.apache.spark.sql.functions.size
	-- We are importing a package for checking the size function with the array.
df.select(size(split(col("Description"), " "))).show(2)
	-- Here first we are splitting the decription columna and then by using size function we can determine the size of the array which we have created out of the description column.
import org.apache.spark.sql.functions.array_contains
	-- Here we are using the import statement to use a function
df.select(array_contains(split(col("Description"), " "), "WHITE")).show(2)
	-- Here we are spliting the description column and checking with whole array which we have created by using array_contains and the element for which we are checking and in the above case it is white.
SELECT array_contains(split(Description, ' '), 'WHITE') FROM dfTable
	-- SQL way of representing the same.
	-- However this not gonna solve our problem out thing was to solve the complex type by putting everything into a row.
	-- This can only by using the explode function .
	-- the explode function will take a column which contains of arrays and put then into different rows per value in the array.
Below thing will explain everything easily
 	
 "Hello World",Other Column __Split___________> ["Hello","World"],"Other Column" _____Explode__> "Hello","Other column"
 																								 "World","Other column"			

import org.apache.spark.sql.functions.{split, explode}
	-- Here we are importing the functions to implement them.
df.withColumn("splitted", split(col("Description"), " ")).withColumn("exploded", explode(col("splitted"))).select("Description", "InvoiceNo", "exploded").show(2)	
	-- Here we are using the WithColumn and creating a new column name splitted and then assigning the splitted column with the split(Description) and this spliited column noe containd the array of the word of description columns 
	-- and the after that we are creating another column with nale explodded and then we are filling that with the Explod of splitted columns and then we are selecting 3 columns Description InvoiceNumber and Explode.
SELECT Description, InvoiceNo, explodedFROM (SELECT *, split(Description, " ") as splitted FROM dfTable)LATERAL VIEW explode(splitted) as exploded
	-- SQL way of writting the same.
import org.apache.spark.sql.functions.map
	-- Here we are importing a package to use the map function.
df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).show(2)
	-- select of map(Descrioption, Invoice) alias complex_map and this complex map column will have key , value pairs and the key would be Description column and the value would be the InvoiceNo.
SELECT map(Description, InvoiceNo) as complex_map FROM dfTable WHERE Description IS NOT NULL		
	-- SQL way of representing the same.
df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).selectExpr("complex_map['WHITE METAL LANTERN']").show(2)	
	-- Here we are checking the key in the complex by using selectExpr if that is unable find anything as of such like WHITE METAL LANTERN this is gonna return a null.
df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).selectExpr("explode(complex_map)").show(2)	
	-- We can also Explode on the Map (key,value) pairs the result it will also splitted as number of rows in the columns.
val jsonDF = spark.range(1).selectExpr("""'{"myJSONKey" : {"myJSONValue" : [1, 2, 3]}}' as jsonString""")	
	-- Here we are creating a Jasonnhere 
You can use the get_json_object to inline query a JSON object, be it a dictionary or array. You can use json_tuple if this object has only one level of nesting:	
import org.apache.spark.sql.functions.{get_json_object, json_tuple}
	-- Here we are implementing the above package to use the above functions.
jsonDF.select(get_json_object(col("jsonString"), "$.myJSONKey.myJSONValue[1]") as "column",json_tuple(col("jsonString"), "myJSONKey")).show(2)	
	-- We use getJasonObject when we have multiple nesting for and object but if we have single nesting we use jason tuple.
jsonDF.selectExpr("json_tuple(jsonString, '$.myJSONKey.myJSONValue[1]') as column").show(2)	
	-- SQL way of writting the same.
import org.apache.spark.sql.functions.to_json
	-- Here we are importing a statement to use that function.
df.selectExpr("(InvoiceNo, Description) as myStruct").select(to_json(col("myStruct")))	
	-- We can also convert the StructType into Jason by using to_Jason function like above	
	-- First we are creating a struct type column and naming the as column an converting that into jaon by using to_Jason.
import org.apache.spark.sql.functions.from_json
import org.apache.spark.sql.types._
	-- We are importing the packages that are required
val parseSchema = new StructType(Array(new StructField("InvoiceNo",StringType,true),new StructField("Description",StringType,true)))		
	-- Here we are defining a schema and assigning it to a varaible 
df.selectExpr("(InvoiceNo, Description) as myStruct").select(to_json(col("myStruct")).alias("newJSON")).select(from_json(col("newJSON"), parseSchema), col("newJSON")).show(2)
	-- Here we are using selectExpr and the we are converting InvoiceNo and Description as my columns and then naming it as mystruct
	-- Then we are using to_jason function and converting the mystruct to jason 
	-- then we are using alias and naming it as newJSON
	-- then we are using select function and from_jason functions from newJason column and then with the schema which we assigned to the varaible parseschema. 
	-- the we are performing shoe on the newJason.
val udfExampleDF = spark.range(5).toDF("num")
	--  we are creating a DF with range 5
def power3(number:Double):Double = number * number * number
	-- Here we are creating a simple UDF which converts the given number into its power of 3.		
power3(2.0)
	-- We will passing the number like above.
	-- In this UDF this work as expected but here we have restrictions on the datatype which means that the input must be specific and there shouldn't be null.
	-- now that we have registered the function and tested them we need to register them with spark and then we can use them on all machines so here spark will serialize the function move it overnetwork and make it available to all network executors.
	-- this happens regards less of language.
	-- When we have the function we have 2 different things that occur essentially .
	-- If the function is written in scala and jave we can use it with JVM.
	-- that means there will be a little performance penalty aside from th fact that we cant take advantage of code generation caabalities for which spark had for built in functions.
	-- There will be performance issue if we create or use lot of objects 
	-- If the function is written in python there will be a different thing running spark will run python process on all slave nodes and the seralize the data in a way python understand and run all the processs and the result is give to the JVM and spark
	-- Starting the python process is expensive but the real cost is when we are serializing the data to python. This is costly for 2 reasons:
		1.it is expensive as computation but also 2.once the data enters the python code spark have nor control over the memory of the worker nodes.
	-- This mean it can cause a work to fail when it comes to resources level.
	-- So it is highly recommended to write the functions in scala and java.
import org.apache.spark.sql.functions.udf
	-- here we are importing a package and then using a function from that
val power3udf = udf(power3(_:Double):Double)
	-- Here we are defining and declaring it as DataFrame
	-- Now we use this as anyother DataFrame.
udfExampleDF.select(power3udf(col("num"))).show()
	-- here we are using the simple UDF like a normal data frame.
spark.udf.register("power3", power3(_:Double):Double)
	-- Regeistering UDF in spark SQL function.
udfExampleDF.selectExpr("power3(num)").show(2)
	-- using it on a DataFrame
	-- If we are writting a UDF as a DataFRame that only be used with the DataFrame that cann be used with some random string or naything of that which is not a DataFrame.
	-- We can write scala functions and use it Pythons and we write python function and we use it as function in any language as well.
	-- One thing we can ensure that our functions are working correctly by making sure we are giving a return type to that function.
	-- As we saw in the beggining the spark maintains it own type information which doesn't align exactly with the pythons types so to avoid complication its better define our return type of our function when we define it 
	-- It is important to note the specifying the return type is not nessacary but it is best parctice.
	-- If we Specify a type for return value and the value which is returned is not as the type which we have specified spark will not throw and error but it simply return null.
	-- This is because the range creates a integers. When integer are operated on in python, python  won't convert them into floats , therefore we see null. 
	-- We can remedy this by ensuring that our Python function returns a float instead of an integer and the function will behave correctly.
CREATE TEMPORARY FUNCTION myFunc AS 'com.organization.hive.udf.FunctionName'
	-- Registering a UDF using SQL but before that we have enable the hive support.
	-- Additionally we can register this as a permanent function in hive metastore by chnaging it from temprorary to permanent.
val df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/retail-data/all/online-retail-dataset.csv").coalesce(5)
	-- Here we are loading the data form the path and specifiyong partition 5.
df.cache()
	-- We are cacheing the dataframe which we have created.
df.createOrReplaceTempView("dfTable")
	-- Creating a temptable for SQL acess.
df.count() 
	-- Count is basic aggerigation and is performed on the entire dataset 
Count():
	-- The first function worth going over is count, except in this example it will perform as a transformation instead of an action					
	-- specify a specific column to count, or all the columns by using count(*) or count(1) to represent that we want tocount every row as the literal one
import org.apache.spark.sql.functions.count
	-- Here we are importing a package to use a function.
df.select(count("StockCode")).show()
	-- Here we are using a function count on column StockCode.
	-- When we are doing count(*) spark will count the null values as well but when we do count(1) spark will not count the null values.
SELECT COUNT(*) FROM dfTable
	-- SQL way of representing the same.
import org.apache.spark.sql.functions.countDistinct
	-- Here we are importing a function for using the function.
df.select(countDistinct("StockCode")).show()
	-- When we distinct spark will only give distinct group of value and remove of the duplicate.
SELECT COUNT(DISTINCT *) FROM DFTABLE
	-- SQL way of doing the same.
import org.apache.spark.sql.functions.approx_count_distinct
	-- Here we are importing a function to use the function.
df.select(approx_count_distinct("StockCode", 0.1)).show() 
	-- this will give the approximate count of the column this is used for large datasets.
import org.apache.spark.sql.functions.{first, last}
	-- Here we are importing a pacakge for using a function.
df.select(first("StockCode"), last("StockCode")).show()
	-- Here we are selecting the first and last of the DataFrame.
import org.apache.spark.sql.functions.{min, max}
	-- Here we are importing a package for using the above functions.
df.select(min("Quantity"), max("Quantity")).show()		
	-- This will extract maximumna and minimun of the specified function.
SELECT min(Quantity), max(Quantity) FROM dfTable
	-- SQL way of doing the same.
import org.apache.spark.sql.functions.sum
	-- We are importing a pacakge to implement the above functions.
df.select(sum("Quantity")).show()
	-- Here we are doing sum on the specified column.
import org.apache.spark.sql.functions.sumDistinct
	-- We are importing a pacakge for implementing the above functions.
df.select(sumDistinct("Quantity")).show()
	-- this will only sum distinct value sremoving the duplicate values from the column.
import org.apache.spark.sql.functions.{sum, count, avg, expr}
	-- Here we are imorting a package to use above functions.
df.select(count("Quantity").alias("total_transactions"),sum("Quantity").alias("total_purchases"),avg("Quantity").alias("avg_purchases"),expr("mean(Quantity)").alias("mean_purchases")).selectExpr("total_purchases/total_transactions","avg_purchases","mean_purchases").show()
	-- Here we are peforming count sun avg and mean on quantity and then naming them as other columns 	
import org.apache.spark.sql.functions.{var_pop, stddev_pop}
import org.apache.spark.sql.functions.{var_samp, stddev_samp}
	-- We are importing a package to implement a function.
df.select(var_pop("Quantity"), var_samp("Quantity"),stddev_pop("Quantity"), stddev_samp("Quantity")).show()
	-- here we are checking the various applications and aggregations.
import org.apache.spark.sql.functions.{skewness, kurtosis}
	-- Here are we are importing a package to implement the functions.
df.select(skewness("Quantity"), kurtosis("Quantity")).show()
	-- Here we are doing skewness and kurtosis aggerigation.
	-- Skewness measures the asymmetry of the values in your data around the mean, whereas kurtosis is a measure of the tail of data	
	-- These are both relevant specifically when modeling your data as a probability distribution of a random variable			
	-- Although here we won’t go into the math behind these specifically, you can look up definitions quite easily on the internet.
SELECT skewness(Quantity), kurtosis(Quantity) FROM dfTable
	-- SQL way of doing the same.
import org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}
	-- Here are we are importing a package to implement above function.
df.select(corr("InvoiceNo", "Quantity"), covar_samp("InvoiceNo", "Quantity"),covar_pop("InvoiceNo", "Quantity")).show()		
	-- We discussed aggregations here like corr and covar_samp covar_pop
import org.apache.spark.sql.functions.{collect_set, collect_list}
	-- Here we are importing a package to implement the above function.
df.agg(collect_set("Country"), collect_list("Country")).show()
	-- In spark we can perform aggregations on not only numbers but also on complexType.
	-- in the above example we are performing aggreigation on complex datatypes.
df.groupBy("InvoiceNo", "CustomerId").count().show()
	-- Here in this we are grouping the data by Invoice and Customer ID so this step is called grouping 
	-- This grouping will return a RelationalGroupedDataset 
	-- Then we are going a aggregation the RelationalGroupedDataset and that is count 
	-- and after performing count that mean aggregation the output of the action or an aggregation would be a DataFrame.
	-- So here when we are doing it will be counting on the group values like when are grouping by invoice it will group all the invoices with same number and then count how many are there in that group 
SELECT count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId
	-- SQL way of doing the same above grouping and Aggregation on Group
import org.apache.spark.sql.functions.count
	-- Here we are importing apackage to use the function.
df.groupBy("InvoiceNo").agg(count("Quantity").alias("quan"),expr("count(Quantity)")).show()		
	-- Here are we are grouping the column and the performing the aggregation on the column which is result of the same group by 
df.groupBy("InvoiceNo").agg("Quantity"->"avg", "Quantity"->"stddev_pop").show()
	-- Here we are specifying the aggrerigations as maps for easily acessing them. here key would be the column and value would be the aggregation function.
import org.apache.spark.sql.functions.{col, to_date}
	-- Here we are importing a package to implement the above functions.
val dfWithDate = df.withColumn("date", to_date(col("InvoiceDate"),"MM/d/yyyy H:mm"))	
	-- Here we are creating a variable type val and the variable name is dfWithDate 
	-- then we are taking our original df and then we are using withColumn function and creating a different column in our df and then we are name it as date and in date column we are saying to date of column invoiceDate and letting the date function know the format which is in the invoice date so that it convert it into its own date format.
Output:
+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+
|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|
+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+
|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|
|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|
+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+
	-- Now if we see we have created a date column with the date format of to_date 
dfWithDate.createOrReplaceTempView("dfWithDate")
	-- So here we are creating a table to access using SQL commands.
	-- The first step of the window function is to create a window specification. the partition by which we are gonna use below is something which we not learned yet.
	-- Its just similar to something like how we are breaking up the group.
	-- the ordering determined the ordering in the give partition and finally the frame specification say which row should be included in which frame based on its interference with thw current row.
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.col
	-- here we are importing some packages to implement those functions
val windowSpec = Window.partitionBy("CustomerId", "date").orderBy(col("Quantity").desc).rowsBetween(Window.unboundedPreceding, Window.currentRow)			-- Here we are using window function along with partition by customer and date after dividing them onto partitions we then ordering the group element which are generated by partition by using the column Quantity in descending order then we are using rows between function to get the rows between unbounded preceding that mean the preceding and the current row.
	-- Here we didn't use that with any of the dataframes yet we have declared everything in the we wanted and now we are storing that in val variable WindowSpec.
	-- Now we are gonna use aggrigation function to learn more about each customer.
	-- the below example will help us get the mex quantity over all time 	
import org.apache.spark.sql.functions.max
	-- Here we are importing a package to implement above function.
val maxPurchaseQuantity = max(col("Quantity")).over(windowSpec)
	-- Here we are getting the max of the column quantity and that to over window spec which we have declared in the above variable.
	-- This will give the Output as column.Now we can use the above generated column in select statement. Before we will get the purchase quantity rank to do that dense_rank function to determine which date had maximun purchases for every customer. We use dense rank as opposed to rank o avoid gaps when there are tied values.
import org.apache.spark.sql.functions.{dense_rank, rank}
	-- Here we have imported a package to implement that above functions.
val purchaseDenseRank = dense_rank().over(windowSpec)
	-- we are using dense_rank function over windoeSpec and assigning it a variable.
DenseRank:
	-- DenseRank function returns the rank of each row within a resultset partition with no gaps in ranking value.
SyntaxOf DenseRank:
DENSE_RANK ( ) OVER ( [ <partition_by_clause> ] < order_by_clause > ) 		
	-- Here the dense rank is followed by Partition by clause and Order by clause.
<partition_by_clause>
	-- First divides the dataset into partitions and then DenseRank is performed over the DataSet. 
<order_by_clause>
	-- Determines the order in which DenseRank applies to the partition of the column 
	-- If 2 or more rows have same partition value that will result in the 2 rows having the same Rank.
	-- 		
val purchaseRank = rank().over(windowSpec)
	-- We are using rank function over window spec and assigning then a variable.
	-- This above also will return a column which we can use in select statements.
dfWithDate.where("CustomerId IS NOT NULL").orderBy("CustomerId").select(col("CustomerId"),col("date"),col("Quantity"),purchaseRank.alias("quantityRank"),purchaseDenseRank.alias("quantityDenseRank"),maxPurchaseQuantity.alias("maxPurchaseQuantity")).show()	
SELECT CustomerId, date, Quantity,
rank(Quantity) OVER (PARTITION BY CustomerId, date
ORDER BY Quantity DESC NULLS LAST
ROWS BETWEEN
UNBOUNDED PRECEDING AND
CURRENT ROW) as rank,
dense_rank(Quantity) OVER (PARTITION BY CustomerId, date
ORDER BY Quantity DESC NULLS LAST
ROWS BETWEEN
UNBOUNDED PRECEDING AND
CURRENT ROW) as dRank,
max(Quantity) OVER (PARTITION BY CustomerId, date
ORDER BY Quantity DESC NULLS LAST
ROWS BETWEEN
UNBOUNDED PRECEDING AND
CURRENT ROW) as maxPurchase
FROM dfWithDate WHERE CustomerId IS NOT NULL ORDER BY CustomerId
	-- SQL way of representing the dense rank 


val dfNoNull = dfWithDate.drop()
	-- We are just using drop function.
dfNoNull.createOrReplaceTempView("dfNoNull")
	-- Creating a Temporary Table.
SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull GROUP BY customerId, stockCode ORDER BY CustomerId DESC, stockCode DESC
	-- Here we are selecting customerID and StockCode and sum(Quantiy) from the temptable we have created dfNoNull and which are grouped by customerId and stockCode and ordering by CustomerId in desecending order and stockCode by Descending order.
SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode))ORDER BY CustomerId DESC, stockCode DESC	
	-- Here we are doing the same group by column using Grouping sets
val a = spark.sql(""" SELECT CustomerID,StockCode,sum(Quantity) FROM dfTable GROUP BY  GROUPING SETS((CustomerID,StockCode)) """)	
	-- Here we are writting in a simpler way by saying GROUPBY (GROUPING SETS ((CustomerID,StockCode)))
val a = spark.sql(""" SELECT CustomerID,StockCode,sum(Quantity) FROM dfTable GROUP BY  GROUPING SETS((CustomerID,StockCode),()) """)
	-- This is similar to the the query but the only thing different here is we are also getting the total number of Quantity by saying 
	()
val a = spark.sql(""" SELECT CustomerID,StockCode,sum(Quantity) FROM dfTable GROUP BY  GROUPING SETS((CustomerID,StockCode),(StockCode),()) """)
	-- Here also it is the same command but we are doing multiple group by operation in the single Group.
val a = spark.sql(""" SELECT CustomerID,StockCode,sum(Quantity) FROM dfTable GROUP BY  GROUPING SETS((CustomerID,StockCode),(StockCode),()) ORDER BY CustomerId DESC, stockCode DESC """)
	-- Same thing but here we are using the Desc on the columsn which we have generated.
	-- Grouping Sets is only available in SQL Spark If we wann do the same on the DataFrames we are gonna use ROLL UP and CUBE

val rolledUpDF = dfNoNull.rollup("Date", "Country").agg(sum("Quantity")).selectExpr("Date", "Country", "`sum(Quantity)` as total_quantity").orderBy("Date")
	-- So when we say RollUp we are gonna get the entire total of the of Quantity  and the every single and country total Quantity and we are ordering it by Date.
rolledUpDF.show()					 	
	-- Here we are performing a show operation.
dfNoNull.cube("Date", "Country").agg(sum(col("Quantity"))).select("Date", "Country", "sum(Quantity)").orderBy("Date").show()	
	-- Cube is next level of the Rollup in rollup it will perform total of Quantity irrespective of Date and country and total for every data and every country and the quantity of that each group.
	-- Where Cube will do in next level it is gonna give the grand total and count of quantity by date and quantity by Country and count of quantity by Data as well as country.
	Clear Explanation is like below:
			-- The total across all dates and countries
			-- The total for each date across all countries
			-- The total for each country on each date
			-- The total for each country across all dates

Difference between RollUp and Cube:
	-- Rollup is something it performs the sum on only hirerachy and where as roll up performs sum on all possible combinations.
	-- in the above example when we said Rollup of Date and Country it performs the sum on the total quantity Which is the grand total and then also by date country and some of there quantity that is all
	-- If we do Cube(Date and Country) it is gonna do grand total first and the by date and country and only by date and only by country these are the possible combinations possible by cube.
Grouping Function:
	-- Grouping function returns 1 for aggregated and 0 for for not aggregated columns.
	-- Which means when we do roll up when we want know if the aggregation is performed on what columns and if there column1 column2 and column3 if there is aggregation on column1 it return 1 and the other columns column2 and column3 will have 0 and 0 and if there is a aggregation on column then it return 1 and return 0 if there is no aggregation.
What is the use of grouping function in real world?
	-- Suppose when we perform roll up or cube or anything as of such we will have nulls values and if we have null values and if we give the same to end user it will be difficult so to replace the null value with a different word and this will help us give meaning full word in place of null.
	-- Just this will help the end user to understand the details in place of null.
SELECT CASE WHEN GROUPING(Continent) = 1 THEN 'All' ELSE ISNULL(Continent, 'Unknown') END AS Continent, CASE WHEN GROUPING(Country) = 1 THEN 'All' ELSE ISNULL(Country, 'Unknown') END AS Country,CASE WHEN GROUPING(City) = 1 THEN 'All' ELSE ISNULL(City, 'Unknown') END AS City, SUM(SaleAmount) AS TotalSales FROM Sales GROUP BY ROLLUP(Continent, Country, City)		
	-- As we discussed in the above column to make nulls understandable to the end user we are replacing every null which is generated as aggregation with all this will make our task easy and the end useer understands it easily.
	-- We all think that we can also achiver the same thins by using is null. yes we can do that but that would be correct only if the table don't contain anyother null values as of such if it contain null these null which are created during aggregationa nd the null which are already present in the table all of them together will be replaced as the desired word.
	-- When we use the grouping function then we will not have that problem in grouping function the nulls which are generateed due to the aggregation will only be replaced with the key word which we have provided.
	-- When we have null in the Data and we are using Grouping this will be declared as unkwon and the null which are genearated due to aggregation will be replaced by the word we assign using the case.
	-- We can use this grouping function in order to help us with the null when we are uing cubes, rollups and also the GroupSets.
Grouping_id():
	-- Grouping function is something which is only performed on the Single column where as Groupin_id can be performed on group of columns which we do group by on this group by can be of any type like group by generated by Cube and Group by generated by rollup of group set or when we do union all also
	so these columns should be smilar to that of then to processed.
	-- Grouping_id is something that which concatinated all the Grouping() values of certain column and convert the binary values of the certain row in that column and then return the equivalent integer.
	-- When we say Grouping_id(colum1,column2,column3)=Grouping(column1)+Grouping(column2)+Grouping(column3)
		Suppose:
			Column1 		Column2 		Column3
			1 				0 				0
			0 				0 				1
			1 				1  				0
			0 				0 				0
	-- Lets assumed the grouping number of the above columns are like this and when we say group_id(column1,column2,column3) = 1+0+0 = 100 = this will concatinate the number it will not add. once this done from this binary value the group_id is gonna generate a integer by using a formula like below
	if we have binary number as 
				0 												1 							1
			2power2*0(0 to power anythin is 0)					2power1*1					2power0*1(as we know anything to power of 0 is 1)
			result 0 											2 							1
			0 								+ 					2 				+   		1 				=3
	So the integer values for the group_id are calculated as above by using 2 power of the byte number starting for right first will be 0 and if 4 number the byte value will from right would be 0 1 2 3
	We will be calculating from right to left 	
	-- The main thing which we wanna remember here is that the Columns which we are mentinoning the grouping_id should be same as the columns which we are mentioning the GroupBy of cube or rollup or Groupingsets. For example if we say cube(column1,column2,column3) then the group_id should contain the columns from column1 and column2 and column3 if we use anyother columns other than these this is gonna result in inconsistency.
	-- We can use this grouping_id to understan the level of grouping. 
import org.apache.spark.sql.functions.{grouping_id, sum, expr}
	-- Here we are importing a package to perform the above function or use the above functions.
dfNoNull.cube("customerId", "stockCode").agg(grouping_id(), sum("Quantity")).orderBy(expr("grouping_id()").desc).show()	
	-- Here we are using cube and performing groupby and the aggregating by Groupid of cube(customerid and stockcode) and ordering by grouping_id in descending order.
val pivoted = dfWithDate.groupBy("date").pivot("Country").sum()
	-- 	Here we used PIVOT.
Joins:
val customers = Seq(  (1, "Customer_1"), (2, "Customer_2"), (3, "Customer_3")).toDF("id", "login")	
	-- Here we are creating a simple DataFrame 
val orders = Seq((1, 1, 50.0d), (2, 2, 10d),(3, 2, 10d),(4, 2, 10d),(5, 1000, 19d)  ).toDF("id", "customers_id", "amount")	
	-- Here we are creating another simple DF by using seq and chnaging them into df using toDF
Inner Join:
	-- This is one of the simplest join to understand. 
	-- It joins the DataSet only if we have coressponding key matching in both DataSet Example when we see the above DataFrames results it would be like below:

CustomerTable:

+---+----------+
| id|     login|
+---+----------+
|  1|Customer_1|
|  2|Customer_2|
|  3|Customer_3|
+---+----------+

Orders Table:

+---+------------+------+
| id|customers_id|amount|
+---+------------+------+
|  1|           1|  50.0|
|  2|           2|  10.0|
|  3|           2|  10.0|
|  4|           2|  10.0|
|  5|        1000|  19.0|
+---+------------+------+
	-- So lets do a inner join on the above tables and therotically see what happens. When we are performing join using id from the customer table and the customer_id from the order table.
	-- When we do that the customer 3 and the order 5 won't be matched and joined cause there us no customer id 1000 exist in the customers DataSet.
	-- We are joining id from customers table and customer_id from the Orders table together using inner jpin
Cross Join:
	-- Cross Join is premissive then the previous one. In fact the cross join leads to the cartesian of product.But the difference with other types lies in the defination 
	-- To use cross joins we must skip condition on joins columns So we define the join as Dataset1(join)DataSet2
	-- There is alos one more thing that we should take care of spark.sql.crossjoin.enabled must be set to true other wise there will be an exception thrown.
	-- The SQL cross join literally produces the number of rows in first column multiplied by the number of rows in second column.
	-- If there is no where clause used it will result in cartesian product.
	-- Cartesian product is something like  a = {h,t} b = {1,2,3} axb = {(h,1)(h,2)(h,3)(t,1)(t,2)(t,3)(1,h)(1,t)(2,h)(2,t)(3,h)(3,t)} so all the element in amultiplied with b and all the elements in b multiplied with a would be the cartesian product of a and b
	-- Similarly if the cross join is not used with where condition it is gonna produce the Cartesian product 
	-- If supposer we are using where clause with the cross join it is gonn work like inner joins.
Left Outer Joins:
	-- Let outer join is something will only give the result set which have rows from the left data set
	-- So for example let us conisder that we are 2 datasets a and b when we are doing left outer join the rows in the left data set will be matched with the rows in the right data set if there are any matching then it is gonna join the rows and if there some rows in the left data set which do not the rows or the keys which are similar in data set be the columns which are enriched for the rows which have similar keys will be filled with null for the columns which don't have the similar columns.
Right Outer Joins:
	-- Right outer Join is simliar to that of the left on except the rows will remain in the result would be right 
Left Semi Joing:
	-- When we perform this join all the rows which are having similar keys with the right dataset will be returned but the only difference is the left dataset will not be added with thw columns of the right dataset but it will only have the columns which are intially owned by the left data set
Left Anti Join:
	-- It will return the all rows in the left data set which do not have similar values with the right data set in above customers and order table only order 5 will be returned.
val person = Seq((0, "Bill Chambers", 0, Seq(100)),(1, "Matei Zaharia", 1, Seq(500, 250, 100)),(2, "Michael Armbrust", 1, Seq(250, 100))).toDF("id", "name", "graduate_program", "spark_status")	
	-- Here we have created a dataframe with name person 	
val graduateProgram = Seq((0, "Masters", "School of Information", "UC Berkeley"),(2, "Masters", "EECS", "UC Berkeley"),(1, "Ph.D.", "EECS", "UC Berkeley")).toDF("id", "degree", "department", "school")
	-- Here we have created a data frame of the name graduate program
val sparkStatus = Seq((500, "Vice President"),(250, "PMC Member"),(100, "Contributor")).toDF("id", "status")
	-- Here we have created another DataFrame of Name sparkStatus.
persons:
+---+----------------+----------------+---------------+
| id|            name|graduate_program|   spark_status|
+---+----------------+----------------+---------------+
|  0|   Bill Chambers|               0|          [100]|
|  1|   Matei Zaharia|               1|[500, 250, 100]|
|  2|Michael Armbrust|               1|     [250, 100]|
+---+----------------+----------------+---------------+
graduateProgram:
+---+-------+--------------------+-----------+
| id| degree|          department|     school|
+---+-------+--------------------+-----------+
|  0|Masters|School of Informa...|UC Berkeley|
|  2|Masters|                EECS|UC Berkeley|
|  1|  Ph.D.|                EECS|UC Berkeley|
+---+-------+--------------------+-----------+
sparkstatus:
+---+--------------+
| id|        status|
+---+--------------+
|500|Vice President|
|250|    PMC Member|
|100|   Contributor|
+---+--------------+

	-- These are 3 DataFrames that we have created.
person.createOrReplaceTempView("person")
	-- Here we are registering the temtable for persons 
graduateProgram.createOrReplaceTempView("graduateProgram")
	-- Here we are gaian registering the temp table fpr graduate program
sparkStatus.createOrReplaceTempView("sparkStatus")
	-- We are also regesitering temptable for sparkstatus.
val joinExpression = person.col("graduate_program") === graduateProgram.col("id")
	-- Here we are creating a variable which says person.col("graduateProgram") === graduateProgram.col("id") and assigning it to the varaible so that we can use that in joins.	
person.join(graduateProgram, joinExpression).show()	
	-- Here we are joining two DataFrames Person and Graduate program based on the joinExpression.
val newies = spark.sql(""" Select * FROM person JOIN graduateProgram ON person.graduate_program = graduateProgram.id """)
	-- In the above statement we have performed the joins in the DataFrame and here we are performing the join in SQL way and we are joining person and graduateProgram based on the colum graduate_program from the table persons and id from the table Graduate program.
var joinType = "inner"
	-- Here we are declaring what type of join we want in a variable and we will use the same to perform that join on the DataFramed
person.join(graduateProgram, joinExpression, joinType).show()
	-- Here we did the same thing that which we have did before but the only difference here is we are specifying the type of the join we want to perform on both dataframes and in this it would be inner join as we have decalred in the above statement.
val tootsie = spark.sql(""" select * from person INNER JOIN graduateProgram ON person.graduate_program = graduateProgram.id """)	
	-- This is just a SQL way of joining 2 DataSets using Inner join or we can also say tables.
Output:
+---+----------------+----------------+---------------+---+-------+--------------------+-----------+
| id|            name|graduate_program|   spark_status| id| degree|          department|     school|
+---+----------------+----------------+---------------+---+-------+--------------------+-----------+
|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|
|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|
|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|
+---+----------------+----------------+---------------+---+-------+--------------------+-----------+
	-- This is an output for all the above joins that would be normal joins both in Dataframes way and SQL way and then the join on inner join on both SQL way and DataFrames so as they have all outputs same.
joinType = "outer"
	-- here we are working with outer join and we are declaring the same in the variblae
person.join(graduateProgram, joinExpression, joinType).show()
	-- So here we are doing the outer join and we have also used the variable which we have assigned with the outer word.
	-- So here while we are doing the Outer Join based on the column graduate_program in person and id in graduatePrograme table.So the tables are like below:
	 			persons:
+---+----------------+----------------+---------------+
| id|            name|graduate_program|   spark_status|
+---+----------------+----------------+---------------+
|  0|   Bill Chambers|               0|          [100]|
|  1|   Matei Zaharia|               1|[500, 250, 100]|
|  2|Michael Armbrust|               1|     [250, 100]|
+---+----------------+----------------+---------------+
graduateProgram:
+---+-------+--------------------+-----------+
| id| degree|          department|     school|
+---+-------+--------------------+-----------+
|  0|Masters|School of Informa...|UC Berkeley|
|  2|Masters|                EECS|UC Berkeley|
|  1|  Ph.D.|                EECS|UC Berkeley|
+---+-------+--------------------+-----------+				

	-- So when we see the graduateprogram column from persons has 0 1 1 and id in graduateProgram has 0 2 1 . if we see the graduate program has all the matching values of keys in the graduate programe but the id don't have matching value for key 2 in the persons table so instead omitting it the join will consider the values of key 2 also but as there are nor corresponding matching keys in the table persons it is gonna be filled with Null like below:
Output:
+----+----------------+----------------+---------------+---+-------+--------------------+-----------+
|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|
+----+----------------+----------------+---------------+---+-------+--------------------+-----------+
|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|
|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|
|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|
|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|
+----+----------------+----------------+---------------+---+-------+--------------------+----------	
	-- So if we see here clearly the table has been joined and all the keys have there corresponding matching keys but as we see 2 of id column from graduateprogram don't have the matching key in persons table the coressposing values for 2 key in the table person are set to null.
val sweetie = spark.sql(""" SELECT * FROM person FULL OUTER JOIN graduateProgram ON person.graduate_program = graduateProgram.id """)	
	-- This is just the SQL way of writting the same thing which we did with the DataFrames way 
joinType = "left_outer"
	-- Here we are using another type of join that is left outer join and assigning the same to the variable and we are gonna use that join our 2 DataFrames.
graduateProgram.join(person, joinExpression, joinType).show()
	-- So here we are performing a join between graduateProgram and persons
	-- When we perform a join using the left outer join then it is gonna check for all the matching keys from both tables and joins the rows with matching key values but if there no matching keys for left table on the right table then they are not gonna include any un matching keys from the right side but all the keys of left will be included and the right colums associated to the not matching key will declared as null.
Output:
+---+-------+--------------------+-----------+----+----------------+----------------+---------------+
| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|
+---+-------+--------------------+-----------+----+----------------+----------------+---------------+
|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|
|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|
|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|
|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|
+---+-------+--------------------+-----------+----+----------------+----------------+---------------+	
	-- So in graduate program we don't have associated or matching key for 2 in the persons table so the columns which are related to persons for the key 2 are assigned as null.
val pancies = spark.sql(""" SELECT * FROM graduateProgram LEFT OUTER JOIN person ON person.graduate_program = graduateProgram.id	""")
	-- This is the SQL way of joining the Datasets using the left outer join.
joinType = "right_outer"
	-- Here we are using another type of join and that is right_outer join and then we are assigning it a varaible and we can use the same to join 2 DataSets.
person.join(graduateProgram, joinExpression, joinType).show()
	-- We can say the right outer join is exactly the opposit of the left outer join it will check of the matching keys in the both datasets and join them and also will keep all the rows from only right dataframe and the corresponsing rows in the left dataframe for the key of the row which don;t have matching values will be assigned as null.
Output:
+----+----------------+----------------+---------------+---+-------+--------------------+-----------+
|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|
+----+----------------+----------------+---------------+---+-------+--------------------+-----------+
|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|
|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|
|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|
|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|
+----+----------------+----------------+---------------+---+-------+--------------------+-----------+
val cuties = spark.sql(""" SELECT * FROM person RIGHT OUTER JOIN graduateProgram ON person.graduate_program = graduateProgram.id """)		
	-- SQL way of performing the same right outer jpin on 2 Datasets.
joinType = "left_semi"
	-- Here we are using another type of join that is left semi join and performing then asigning the same to the varaible so that we can use it in joining both DataFrames using join.
graduateProgram.join(person, joinExpression, joinType).show()
	-- Semi left joins are bit different from other joins which we have used till now.
	-- so the left semi join compares all the keys which are in the left table to the right table and see if there are any corresponding values if yes then there will be no columns inlucded in the output from the right table and also all the columns of left will also be not included but the the output will only have rows of left table and that too the output only have the values of rows on the left that having matching values on the right.
	-- We can simply say this left semi join will filter out the rows based on the matching from the right table
Output:	
+---+-------+--------------------+-----------+
| id| degree|          department|     school|
+---+-------+--------------------+-----------+
|  0|Masters|School of Informa...|UC Berkeley|
|  1|  Ph.D.|                EECS|UC Berkeley|
+---+-------+--------------------+-----------+	 
 	-- So if we see here the left table is graduateprogram table and the right is persons when perform left semi join the left table that is graduate program has 0 2 1 as key and person table has 0 1 1 as key so as there is no key for matching 2 key in the right column it is gonna drop.
 val gradProgram2 = graduateProgram.union(Seq((0, "Masters", "Duplicated Row", "Duplicated School")).toDF())			
	-- SO here we are just assigning another row to the graduate program table by performing union on the graduate program table and assigning it to a new variable gradProgram2 	
gradProgram2.createOrReplaceTempView("gradProgram2")
	-- We are also creating a temporary view for the table so we can acess SQL .
gradProgram2.join(person, joinExpression, joinType).show()
	-- So when we we are performing the same left semi join for the table gradProgram2 and persons now gradProgram 2 has 4 rows with keys 0 0 1 2 and person table have 0 1 1 
	-- In the above case we have matching key for 0 and 1 and nothing for 2 so the all rows on the left which are having matching keys will be included in the output so we have 2 0's and 1 so the out put will have 2 rows of key 0 and row of key 1 like below 
gradProgram2:
+---+-------+--------------------+-----------------+
| id| degree|          department|           school|
+---+-------+--------------------+-----------------+
|  0|Masters|School of Informa...|      UC Berkeley|
|  2|Masters|                EECS|      UC Berkeley|
|  1|  Ph.D.|                EECS|      UC Berkeley|
|  0|Masters|      Duplicated Row|Duplicated School|
+---+-------+--------------------+-----------------+

persons:
+---+----------------+----------------+---------------+
| id|            name|graduate_program|   spark_status|
+---+----------------+----------------+---------------+
|  0|   Bill Chambers|               0|          [100]|
|  1|   Matei Zaharia|               1|[500, 250, 100]|
|  2|Michael Armbrust|               1|     [250, 100]|
+---+----------------+----------------+---------------+
Output:
+---+-------+--------------------+-----------------+
| id| degree|          department|           school|
+---+-------+--------------------+-----------------+
|  0|Masters|School of Informa...|      UC Berkeley|
|  1|  Ph.D.|                EECS|      UC Berkeley|
|  0|Masters|      Duplicated Row|Duplicated School|
+---+-------+--------------------+-----------------+
	-- We can simply say all the rows of table which is in the left will be included in the output provided they have a matching key in the correspinding data set or table.
val banty = spark.sql(""" SELECT * FROM gradProgram2 LEFT SEMI JOIN person ON gradProgram2.id = person.graduate_program	""")
	-- This is the SQL way of performing the Left semi join.
joinType = "left_anti"
	-- Here we are perfoming another type of join on 2 datasets that is left anti join
graduateProgram.join(person, joinExpression, joinType).show()	
	-- left anti joins are opposite to left semi join 
	-- As we know the left semi join will not include any matching key which are in the right table or Dataframe. they only compare the values and see of they have matching in the right dataframe 
	-- So here the only colums of the right dataframe will be included and that too only the row that don't have matching corresponding key.
Output:
+---+-------+----------+-----------+
| id| degree|department|     school|
+---+-------+----------+-----------+
|  2|Masters|      EECS|UC Berkeley|
+---+-------+----------+-----------+	
	-- So as we see in the graduate program table we don't have matching key for 2 so the row which don't have matching keys in the left of dataframe will be retained in the output and the all the remaining rows will be discarded.
val buntyi = spark.sql(""" SELECT * FROM graduateProgram LEFT ANTI JOIN person ON graduateProgram.id = person.graduate_program	""")
	-- This the simple SQL way of performing the left anti join on the 2 tables or simply we can say tables.
val bani = spark.sql(""" SELECT * FROM graduateProgram NATURAL JOIN person	""")
	-- SQL way of performing the natural join.
joinType = "cross"
	-- Here we are using another type of join and that is cross join and the output of this join would be cartesian of both the dataframes of all the rows.
	-- Here we have assigned it to a varaiable so that we can use that perfrom the join.
graduateProgram.join(person, joinExpression, joinType).show()
	-- In cross join every row in the left column will be joined with every row in the right column 
Output:
+---+-------+--------------------+-----------+---+----------------+----------------+---------------+
| id| degree|          department|     school| id|            name|graduate_program|   spark_status|
+---+-------+--------------------+-----------+---+----------------+----------------+---------------+
|  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|
|  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|
|  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|
+---+-------+--------------------+-----------+---+----------------+----------------+---------------+
	-- Here we not checking anything but we are just joining then with out anu condition.
val banty = spark.sql(""" SELECT * FROM graduateProgram CROSS JOIN person ON graduateProgram.id = person.graduate_program """)
	-- This is nothing but the SQL way of perfroming the cross join on the tables 
person.crossJoin(graduateProgram).show()
	-- This we performing cross join on the Dataframes with out any rescriction like before saying to perform only if the have equal keys so here every row in the every table will be multiplied with each other.
Output:
+---+----------------+----------------+---------------+---+-------+--------------------+-----------+
| id|            name|graduate_program|   spark_status| id| degree|          department|     school|
+---+----------------+----------------+---------------+---+-------+--------------------+-----------+
|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|
|  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|
|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|
|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|
|  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|
|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|
|  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|
|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|
|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|
+---+----------------+----------------+---------------+---+-------+--------------------+-----------+
	-- Every row in the table on left is multiplied with the every on the right table.
val toony = spark.sql(""" SELECT * FROM graduateProgram CROSS JOIN person """)
	-- This is SQL way of performing cross join without on condition on the keys.
import org.apache.spark.sql.functions.expr
	-- Here we are importing a package to perform the above function or use them 
person.withColumnRenamed("id", "personId").join(sparkStatus, expr("array_contains(spark_status, id)")).show()	
	-- Here we are performing a inner join as we have not mentioned any join
Output:
+--------+----------------+----------------+---------------+---+--------------+
|personId|            name|graduate_program|   spark_status| id|        status|
+--------+----------------+----------------+---------------+---+--------------+
|       0|   Bill Chambers|               0|          [100]|100|   Contributor|
|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|
|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|
|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|
|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|
|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|
+--------+----------------+----------------+---------------+---+--------------+		
val tan =  spark.sql(""" SELECT * FROM(select id as personId, name, graduate_program, spark_status FROM person)INNER JOIN sparkStatus ON array_contains(spark_status, id""")
	-- Thid just SQL way of representing the same.
val gradProgramDupe = graduateProgram.withColumnRenamed("id", "graduate_program")
	-- Here we are creating another Dataframe which has similar names as graduate program and the result would be :
Output:
+----------------+-------+--------------------+-----------+
|graduate_program| degree|          department|     school|
+----------------+-------+--------------------+-----------+
|               0|Masters|School of Informa...|UC Berkeley|
|               2|Masters|                EECS|UC Berkeley|
|               1|  Ph.D.|                EECS|UC Berkeley|
+----------------+-------+--------------------+-----------+
val joinExpr = gradProgramDupe.col("graduate_program") === person.col("graduate_program")		
	-- Here we are performing a join based on the condition which is column graduate program from gradprogramDupe and column graduate program from person.
person.join(gradProgramDupe, joinExpr).show()	
	-- So here as we have 2 columns with same names although they are in differen data frames and now we are joining them on that and the out put will have 2 columns with same name i.e. duplicate columns.
Output:
+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+
| id|            name|graduate_program|   spark_status|graduate_program| degree|          department|     school|
+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+
|  0|   Bill Chambers|               0|          [100]|               0|Masters|School of Informa...|UC Berkeley|
|  2|Michael Armbrust|               1|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|
|  1|   Matei Zaharia|               1|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|
+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+
	-- If we see we have 2 graduate_program columns in output.
person.join(gradProgramDupe, joinExpr).select("graduate_program").show()
	-- So here we are refereing to the graduate_program so here comes the proble because spark cannot understand to which column we are referring to.
	-- It gives and error stating this " Reference 'graduate_program' is ambiguous, could be: graduate_program, graduate_program.;"
How to solve the problem:we have 2 different approches.
1.Different join Expression:
	-- When we have 2 keys that have same name the easiest way to fix it is to change the join xpression from boolean expression to string or seq
	-- This will automatically remove the one column for us during the join.
person.join(gradProgramDupe,"graduate_program").select("graduate_program").show()
	-- Here we are referring to the column directly instead of giving a condition like in val joinExpr = gradProgramDupe.col("graduate_program") === person.col("graduate_program") which will return a boolean value 
Output:
+----------------+
|graduate_program|
+----------------+
|               0|
|               1|
|               1|
+----------------+
2.Dropping the column after the join:
	-- Here we will drop the offending or duplicate columns after the join.
	-- This is possible by only referring the column via the original source dataframe.we can do this if the join uses the same key name and or the output have same column names.
person.join(gradProgramDupe, joinExpr).drop(person.col("graduate_program")).select("graduate_program").show()	
	-- So here we are performing the join and dropping the column based on the dataframe like in the above statement we will be dropping the column in the dataframe persons and so now the result will have the column if the dataframe graduate program.
	-- This will go out with no error and the result will be same as above.
val joinExpr = person.col("graduate_program") === graduateProgram.col("id")
	-- here we are giving a statement which will return a boolen 
person.join(graduateProgram, joinExpr).drop(graduateProgram.col("id")).show()
	-- So here we are perfoming the join on boolean expression and it is gonna return the output with a duplicate column but we are dropping one of them in the above case the columsn which is in dataframe persons and making it easy to referrable.
Output:
+---+----------------+----------------+---------------+-------+--------------------+-----------+
| id|            name|graduate_program|   spark_status| degree|          department|     school|
+---+----------------+----------------+---------------+-------+--------------------+-----------+
|  0|   Bill Chambers|               0|          [100]|Masters|School of Informa...|UC Berkeley|
|  2|Michael Armbrust|               1|     [250, 100]|  Ph.D.|                EECS|UC Berkeley|
|  1|   Matei Zaharia|               1|[500, 250, 100]|  Ph.D.|                EECS|UC Berkeley|
+---+----------------+----------------+---------------+-------+--------------------+-----------+
	-- So the output is gonna show the one column here as we dropped the duplicate column 
3.Renaming the column before join:
	-- we can avoid this issue completely if we rename on of the columns before performing the join.
val gradProgram3 = graduateProgram.withColumnRenamed("id", "grad_id")
	-- Here we are renaming the column graduate_program from the dataframe graduateProgram and name the new dataframe as gradProgram3.
Output:
+-------+-------+--------------------+-----------+
|grad_id| degree|          department|     school|
+-------+-------+--------------------+-----------+
|      0|Masters|School of Informa...|UC Berkeley|
|      2|Masters|                EECS|UC Berkeley|
|      1|  Ph.D.|                EECS|UC Berkeley|
+-------+-------+--------------------+-----------+
val joinExpr = person.col("graduate_program") === gradProgram3.col("grad_id")
	-- Same old expression for mention the condition of joining the 2 dataframes.
person.join(gradProgram3, joinExpr).show()
	-- No issue will be popped even if have rows of the 2 columns are same because the column names are different as we made sure by renaming the column which are same.
val joinExpr = person.col("graduate_program") === graduateProgram.col("id")
	-- here we are doing just the usual thing 
person.join(graduateProgram, joinExpr).explain()
-- Here when we do this we can see what join will spark perfomr on this case ie is like below.
Output:
== Physical Plan ==
*(1) BroadcastHashJoin [graduate_program#5582], [id#5610], Inner, BuildLeft
:- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[2, int, false] as bigint)))
:  +- LocalTableScan [id#5580, name#5581, graduate_program#5582, spark_status#5583]
+- LocalTableScan [id#5610, degree#5611, department#5612, school#5613]
	-- Here spark has performed brodcast has join 
	-- We can also explicitly give optimiser a hint stating that join by using broadcast 
import org.apache.spark.sql.functions.broadcast
	-- Here we are importing a package to perform specific fucntions
val joinExpr = person.col("graduate_program") === graduateProgram.col("id")
	-- Same old.
person.join(broadcast(graduateProgram), joinExpr).explain()
	-- Here we are requesting optimiser to perform join brodcast join.
	-- in this case it is not gonna change because this join is already performed as broadcast join.
SELECT /*+ MAPJOIN(graduateProgram) */ * FROM person JOIN graduateProgram ON person.graduate_program = graduateProgram.id					
	-- SQL way tellling optimiser too use broadcast join.
	-- This doesn’t come for free either: if you try to broadcast something too large, you can crash your driver node (because that collect is expensive). This is likely an area for optimization in the future.
----------------------------------	
DataFrameReader.format(...).option("key", "value").schema(...).load()
	-- This is a core structure of reading a datasourcer
	-- We will use this format to read from all of our DataSources. Format is optionl cause by default spark reads everything in parquet format.
	-- (key,values) are somthing which will help the spark to understand how we wann read our data.
	-- Schema is also option we can either take the schem available in the data or we can define our own schema 
Spark.read:
	-- This is something we will use to read the data from read stream using SparkSession.
After DataFrame reader we specify several option like:
	-- format
	-- schema
	-- readmode 
	-- series of othe options 
spark.read.format("csv").option("mode", "FAILFAST").option("inferSchema", "true").option("path", "path/to/file(s)").schema(someSchema).load()			
	-- here is the example of how we are doing that . There are variety of ways in which we can use options.
DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()
	-- Core structure of writting API is as above 
	-- We use this format to write to all of our datasources. format is optional because spark uses default as parquet format.
	-- Option gains allows us to configure how to write our give data partition BY , bucke by sort bY works only for file based datasources 
	-- We can use them to get specific layout of the file at destination.
Basics of writting Data:
	-- Th efindation of writting data is quite similar to that of reading data.
	-- Instead of DataFrame Reader we have DataFrame Writer.
	-- Because we always need to write out some given datasource we acess dataframe writer via write attribute.
DataFrame.write:
	-- After we write the DataFrame writer then we will specify teh format and then options and then save. at minum we will need to give a path to store the DataFrame.
dataframe.write.format("csv").option("mode", "OVERWRITE").option("dateFormat", "yyyy-MM-dd").option("path", "path/to/file(s)").save()		
	-- this is the simple way of writting a Dataframe.
SaveModes:
	-- append Appends the output files to the list of files that already exist at that location
	-- overwrite Will completely overwrite any data that already exists there
	-- errorIfExists Throws an error and fails the write if data or files already exist at the specified location
	-- ignore If data or files exist at the location, do nothing with the current DataFrame	
	-- The default is error if exist 
CSVFiles:

Read/Write 		Key 							Potentialvalues 						Defaults 						description
Both 			sep 							Anysingle String character					,					The single character that is used as 
																												seperator or each field and value.
Both 			Header 							true,false 								false 					A boolean flag that declares weather 																													the first lines in the columns are 	
																												column name or not.			
Read 			Escape							Any String Character 						/					The character spark should use to escape
																												the special characters in the file.
Read 			inferschema 					true,false 								false 					Specifies weather spark should decides																													column types when reading file.
Read 			ignoreLeadingWhiteSpace 		true,false 								false 					Declares weather the reading spaces from
																												the values which are read to be skipped
Read 			ignoreTrailingWhiteSpace 		true,false 								false 					Declare weather the traking space from 
																												the values which are read to skipped or no										
Both 			nullValue 						AnyStringCharacter 						" "						Declares what character represents null 																												value in the file
Both 			nanValue 						AnySteingCharacter 						NaN 					Declare what character represents the 																													NaN value in the file.
Both 			positiveInf 					AnyStringorCharacter					Inf 					Declares what charactwe represents a 																													positive infinity value
Both 			negativerInf 					AnyStringorCharacter 					-Inf 					Decalres what character represents a 	
																												Negative infinity value
Both 			compression or codec 			None,Uncompressed,bzip2,deflate
												gzip,lz4,or snappy 						none 					Declares what compression codec spark 																	should read or write the file
Both 			DateFormat 						AnyStringorCharacter that confirms
												to java SimpleDataFormat 				yyyy-MM-dd 				Decalres the date format for nay columns																that are datatype
Both 			timeStampFormat 				AnyStringorCharacter that confirms		yyyy-MM-dd'T'
												to java SimpleDataFormat				HH.mm.ss.ssszz 			Declares the data format for any columns																that are DataTypes
Read 			maxColumns 						AnyInteger 								20480					decalres the Maximum of number of 																															columns in a file
Read 			MaxCharsPerColumn 				AnyInteger 								1000000					Declares tje Maximum number of chars in 																												a column.								
Read 			escapeQuotes 					true,false 								true 					Decalres if Spark should escape the qu																													otes which are find in the line or not
Read 			maxMalformedLogPerPartition 	AnyInteger 								10 						Sets the maximum number of malformed 
																												rows spark will log for each partition
																												malformed record beyond this number
																												will be ignored.
Write 			quoteAll 						true,false 								false 					Specifies if all values should be 
																												enclosed n quotes as oppsed to just scaping values that have quote chars
Read 			multiline 						true,false 								false 					This option allows to read multiline 																													CSV files where each logical row in the
																												CSV file might span multiple rows in the
																												file itself									
Reading CSV file:
	-- To read a csv while we have to create a dataframe reader like we do for another other format 
	-- Here we specify the type as CSV
spark.read.format("csv")
	-- Here we are specifying the read format as CSV
spark.read.format("csv").option("header", "true").option("mode", "FAILFAST").option("inferSchema", "true").load("some/path/to/file.csv")					-- So the basic way of reading a file would be like above first we have mentioned the format and then we are using option multiple times for like setting the mode to failfast and you know also using the inferschem as true and using the load function to load the file and this we will be able to read a CSV file
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
	-- Here we are importing a package to implement the above functions
val myManualSchema = new StructType(Array(new StructField("DEST_COUNTRY_NAME", StringType, true),new StructField("ORIGIN_COUNTRY_NAME", StringType, 	true),new StructField("count", LongType, false)))																											-- Here we are defining a manualschema and we will assign the same to a DataFrame like below 
spark.read.format("csv").option("header", "true").option("mode", "FAILFAST").schema(myManualSchema).load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv").show(5)
	-- Here we reading a file in format CSv and the we are using options asking the spark to conisder the header as the columns names to the dataframe and then we are setting manualSchema with the variable which we have declared and then we are loading this filr by providing the path in load function.
	-- Spark fails the job at execution time rather than DataDefination time due to lazy evaluation.
Writting CSV file:
	-- Just as for reading we also have many options for writing the data.
val csvFile = spark.read.format("csv").option("header", "true").option("mode", "FAILFAST").schema(myManualSchema).load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv")																		-- Here we are reading a CSV file and creating a dataframe csvFile.
csvFile.write.format("csv").mode("overwrite").option("sep", "\t").save("/Users/kartikeyan/Desktop/tmp/my-tsv-file.tsv")										-- Here we are writing a csv file into a tsv file very easily.
JSON files:
	-- In spark when we are referring to a JSON file we will be referring to a line delimited JSON file.
	-- So we read Json file in single line delimited file and then multline delimited file and we mention that and make spark understand it by saying multiline true and multiline false when we say multiline true the Json will be read in multiple rows of spark but when we say multiline flase the entire json file will be considered as on whole row.
	-- Line delimited is more stable is spark because it will allow us append a file with new record which is what recommended to use in spark.
JSON options:

Read/Write 		key 						potentialvalues 				Default 				Description
Both 			Compressionorcodec			None,uncompressed,										Declared What copression codec spark should use to 
											bzip2,deflate 					None 					Read or write the file.
Both 			dateFormat 					Anystringconformsjave
											SimpleDataFormat 				yyyy-mm-dd 				Declares Date format for any columns that declare t
																									he date format
Both 			timeStamp 						""								""					Decalres time stamp format for any column that 																											declare time stamp format 
Read 			primitiveAsString 			true,false 						false 					Infers all primitive values as string type			
Read 			allowComments 				true,false 						false 					Ignores all java/c++ type of commands in Jsonfiles
Read 			allowUnQuotedFilesNames 	true,false 						false 					Allows unquoted Json fields name
Read  			allowSingleQuotes 			true,false 						true 					Allows single quotes along with double quotes for jf
Read 			allowNumericReadingZero 	true,false 						false 					Allows leading zeros in number
Read 			allowsBackslashEscapingany
				Character 					true,false 						false 					Allows accepting quotes of all characters using 
																									backslash quoting mechanism
Read 			columnNameOfCorruptedRecord AnyString 						imports 				related to malformed file
Read 			multiline 					true,false 						false 					Allows for reading in nonline delimited Json files.

spark.read.format("json")
	-- reading a Json file using read format 
spark.read.format("json").option("mode", "FAILFAST").schema(myManualSchema).load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/json/2010-summary.json").show(5)	
	-- Here we reading a Json file while mentioning many type fo option which we have in spark and loading file from a path using load function.
csvFile.write.format("json").mode("overwrite").save("/Users/kartikeyan/Desktop/tmp/my-json-file.json")	
	-- Here we are writing the same Json file into to a path which we have mationed and the name which we want the json to be.
Parquet files:
	-- parquet is open source columns oriented datatore that provides variety of storage optimisations espcially for analytical workload.
	-- It provides a columnar compression which saves the storage space and allows for reading individual columns instead of an entire files.This the file format that works exceptionally well with apache spark and infact this the default file format spark has and uses.
	-- So spark recommends to write any output file in parquet format which is alot better than that of csv and json
	-- Another advanatage of parquet is that it supports complex types. This mean if our column is struct array map or any complex type we will be able to read without a problem 
spark.read.format("parquet")
	-- Read format for parquet 
Reading parquet files :
spark.read.format("parquet").load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/parquet/2010-summary.parquet").show(5)
	-- this is how we will be reading a parquet format 
Parquet Options:

Read/write 					key 				Potential Values 						Default 				Descriptions
Write 						Compression
							codec 					bla bla 							None 					Declares what compression codec spark 
																												should use while reading a parquet
Read 						MergeSchema 			true,false 							bla bla 				You can incrementally add columns to
																												newly written Parquet files in the same
																												table/folder. Use this option to enable or disable this feature
csvFile.write.format("parquet").mode("overwrite").save("/tmp/my-parquet-file.parquet")
	-- This is the way of writting the parquet file 
ORC Files:
	-- ORC is a self-describing type aware columnar file format designed for hadoop workloads. Is is optimised for strong streamin reads but with integrataed support for finding required rows quickly 
	-- ORC actually don't have option for reading becuase spark understand ORC quite well. For say ORC and parquet are quite similar parquet is further optimised for spark where as ORC is further optimised for Hive
spark.read.format("orc").load("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/orc/2010-summary.orc").show(5)
	-- here we are reading a ORc file
csvFile.write.format("orc").mode("overwrite").save("/tmp/my-json-file.orc")
	-- Writting in ORC format 
SQL Databases:
	-- SQL datasources are one of the most powerful connectors because there are varaiety of systems to which you can connect. For an instance we can connect to MySQL Database, PostgreSQL Database, Oracel DataBase.
	-- we can also connect to SQL Lite 	
To read and write from the databases we need to do 2 things:
	1.Include java connectivity driver on Database (JDBC) for our particular database on the spark classpath 
	2.Should provide proper jar for driver itself
spark-shell --driver-class-path /Users/kartikeyan/Desktop/sqlite-jdbc-3.8.6.jar --jars /Users/kartikeyan/Desktop/sqlite-jdbc-3.8.6.jars
	-- If we want to run the sqllite database we are gonna do like above.

JDBC data source options:
url 											The JDBC URL to which to connect. The source-specific connection properties can be specified in the 													URL; for example, jdbc:postgresql://localhost/test?user=fred&password=secret.
dbtable											The JDBC table to read. Note that anything that is valid in a FROM clause of a SQL query can be
												used. For example, instead of a full table you could also use a subquery in parentheses.
driver 											The class name of the JDBC driver to use to connect to this URL.

partitionColumn,
lowerBound, upperBound							If any one of these options is specified, then all others must be set as well. In addition,
												numPartitions must be specified. These properties describe how to partition the table when reading
												in parallel from multiple workers. partitionColumn must be a numeric column from the table in
												question. Notice that lowerBound and upperBound are used only to decide the partition stride, not for
												filtering the rows in the table. Thus, all rows in the table will be partitioned and returned. This option applies only to reading.
numPartitions 									The maximum number of partitions that can be used for parallelism in table reading and writing. This
												also determines the maximum number of concurrent JDBC connections. If the number of partitions
												to write exceeds this limit, we decrease it to this limit by calling coalesce(numPartitions) before 
												writing.
fetchsize										The JDBC fetch size, which determines how many rows to fetch per round trip. This can help
												performance on JDBC drivers, which default to low fetch size (e.g., Oracle with 10 rows). This
												option applies only to reading.
batchsize										The JDBC batch size, which determines how many rows to insert per round trip. This can help
												performance on JDBC drivers. This option applies only to writing. The default is 1000.
isolationLevel									The transaction isolation level, which applies to current connection. It can be one of NONE,
												READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to
												standard transaction isolation levels defined by JDBC’s Connection object. The default is
												READ_UNCOMMITTED. This option applies only to writing. For more information, refer to the
												documentation in java.sql.Connection.
truncates 										This is a JDBC writer-related option. When SaveMode.Overwrite is enabled, Spark truncates an
												existing table instead of dropping and re-creating it. This can be more efficient, and it prevents the
												table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as
												when the new data has a different schema. The default is false. This option applies only to writing.
createTableOptions								This is a JDBC writer-related option. If specified, this option allows setting of database-specific 													table and partition options when creating a table (e.g., CREATE TABLE t (name string)ENGINE=InnoDB). 													This option applies only to writing.
createTableColumnTypes							The database column data types to use instead of the defaults, when creating the table. Data type
												information should be specified in the same format as CREATE TABLE columns syntax (e.g.,
												“name CHAR(64), comments VARCHAR(1024)”). The specified types should be valid Spark SQL
												data types. This option applies only to writing.	

Reading from SQL Databases:
	-- When it comes to reading a SQL nothing changes when compared to other.
val driver = "org.sqlite.JDBC"
	-- here we are assigning the sqlite driver to a varaible 
val path = "/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/jdbc/my-sqlite.db"
	-- Giving the path of the file 
val url = s"jdbc:sqlite:/${path}"
	-- reading the path in concerened URL 
val tablename = "flight_info"
	-- here we are giving the table name which we want to read 
import java.sql.DriverManager
	-- Here we are importing a package 
val connection = DriverManager.getConnection(url)
	-- checking connection and assigning to variable
connection.isClosed()
	-- Finally checking for the connection
connection.close()	
	-- finally clossing
	-- If the connection succeds with out any error we will be reading a dataframe from the SQL table.
val dbDataFrame = spark.read.format("jdbc").option("url", url).option("dbtable", tablename).option("driver", driver).load()			
	-- Here we are reading a table into a dataframe 
dbDataFrame.show()
	-- We will be able to see the DataFrame.	
	-- As we all know SQL lite is very simple and easy to acess and don't have any users in it
spark-shell --driver-class-path /Users/kartikeyan/Desktop/jary/mysql-connector-java-8.0.16.jar --jars /Users/kartikeyan/Desktop/jary/mysql-connector-java-8.0.16.jar
	-- Here if we wanna connect to a database which have you knoe user and password. and also load the corresponding jars so that spark can use them and we did that in the above step.
val pgDF = spark.read.format("jdbc").option("driver", "com.mysql.jdbc.Driver").option("url", "jdbc:mysql://localhost:3306/recipes_database?useLegacyDatetimeCode=false&serverTimezone=UTC").option("dbtable", "recipes_database.recipes").option("user", "root").option("password","light").load()		
	-- Here we are establishing the connection and loading the databases and reading the table from the database
Output:
+---------+--------------+                                                      
|recipe_id|   recipe_name|
+---------+--------------+
|        3|Grilled Cheese|
|        1|         Tacos|
|        2|   Tomato Soup|
+---------+--------------+
dbDataFrame.select("recipe_name").distinct().show(5)
	-- Here we are performing a select operation on DB
	-- This the table that which is literally present in the MysqlDatabase.
QueryPushDown:
	-- First spark makes best effort to filter the data in the database itself before creating a dataframe.
	-- So for example when we have checked the above example spark have pulled only relevant information.
dbDataFrame.select("DEST_COUNTRY_NAME").distinct().explain
	--Spark can actually do better than this on certain queries. For example, if we specify a filter on our
	DataFrame, Spark will push that filter down into the database. We can see this in the explain plan
	under PushedFilters
val pushdownQuery = """(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS flight_info"""
	-- Sql Way
val dbDataFrame = spark.read.format("jdbc").option("url", url).option("dbtable", pushdownQuery).option("driver", driver).load()
	-- Creating a table
	-- Output is a table an when we will query the table we will be query the table which we have filtered from actual table.
dbDataFrame.explain()	
	-- Here we are saying expalin and we will be getting all the details about the table and what is pushed down and all 
Reading from databases in parallel
	-- Spark has an underlying algorithm that can read multiple files into a single partition or conersely read multiple partition out of one file depending on the file type and spliteability of the file type and compression.
	-- the same thing is avaialble for SQL file also but the only concern here we have to do to more manually then we normally do with other files.
	-- Here we have ability to specify the maximum number of partition to be done on a file or something like that.
val dbDataFrame = spark.read.format("jdbc").option("url", url).option("dbtable", tablename).option("driver", driver).option("numPartitions", 10).load()		-- Here if we even mention the number of partition the number of partition will not change cause tha data is very low this can also happen 				oppositely.
dbDataFrame.select("DEST_COUNTRY_NAME").distinct().show()
	-- There so many optimisation which are seen under another API set and we can explicitly push the prediacte down to the SQL table itself.
	-- This optimisation allows you to control certain physical location of certain data in certain partition by's
val props = new java.util.Properties
props.setProperty("driver", "org.sqlite.JDBC")
val predicates = Array("DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'","DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'")
spark.read.jdbc(url, tablename, predicates, props).show()
spark.read.jdbc(url, tablename, predicates, props).rdd.getNumPartitions
	-- So we have achived the partitions thing in a different way 
val props = new java.util.Properties
props.setProperty("driver", "org.sqlite.JDBC")
val predicates = Array(
"DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'",
"DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'")
spark.read.jdbc(url, tablename, predicates, props).count() 
	-- Prediactes which have duplicates.
val props = new java.util.Properties
props.setProperty("driver", "org.sqlite.JDBC")
val predicates = Array(
"DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'",
"DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'")
spark.read.jdbc(url, tablename, predicates, props).count() 

Partitioning based on a sliding window
	-- We are doing partitioning in a different way 
	-- Here we are seeing how we can do partitioning based on predicates.
	-- In this example we will partition based on numerical column called as count and we will also specify the maximum and minimum of the each partition too anything inside of the bound will be in first partition and anything outside of the bound will be in last or the second partition
	-- Spark then quries the database is parallel and you know will return the partition mentiond in the numPartitions.
val colName = "count"
val lowerBound = 0L
val upperBound = 348113L 
val numPartitions = 10
	-- This is we what we did and we will use this later 
spark.read.jdbc(url,tablename,colName,lowerBound,upperBound,numPartitions,props).count() 	
	-- NAHHHH
Writing to SQL Databases
	-- Writting to SQL Databases is just easy as before.
	-- We simply specify the URL and write out the data according to the specified write mode that we want 
TextFiles:
	-- Saprk also allows us to read plain text files 
spark.read.textFile("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/csv/2010-summary.csv").selectExpr("split(value, ',') as rows").show()		
	-- Here we are reading a textfile.
Writting TextFiles:
	-- While writing the text file we need to make sure that there is only one string column otherwise the write will fail
csvFile.select("DEST_COUNTRY_NAME").write.text("/tmp/simple-text-file.txt")
	-- Here we are just writting a text file
	-- If we perform some partitioning on the write we can write more columns.However those columns manifeast as directories. 			
Advanced I/O types:
	-- We saw previously that we can control the parallelism of files that we write by controlling them before writting the files.
	-- We can also control specific data layout by controlling 2 things:
			1.buketing 
			2.Partitioning
Splittable File types and compression:
	-- Certain file formats are fundamentally splittable. This can improve speed because this will saprk to avoid reading the entire file and acess only that parts of the file where we can satisfy the condition.
	-- Additionally if we are using something like Hadoop file System (HDFS) splitting a file provides further optimisation if the file spans multiple blocks In conjection with this there is a need to manage compression
	-- Not all compression schemas are splittable to make it some spark recommonds parquet with gzip
Reading Data in Parallel:
	-- Multiple Executors cannot read from the same file at the same time nessacarily but they can read different files at same time.
	-- Which mean if we have multiple files in a folder they all become each partition for spark and the exectors run parallely on them 	 			
csvFile.repartition(5).write.format("csv").save("/tmp/multiple.csv")
	-- Example when we see the above statement we are setting as 5 partition when we write this we will end up with 5 files inside a folder.
ls /tmp/multiple.csv
	-- this folder will have 5 files because we splitted the file already or we can say 
Partitioning:
	-- Partitioning is a tool which allows us to control where data is store as we write it.
	-- when we write a file to a partitioner or a table we basically encode a column as a folder.
	-- This allow us to skip lot of data when we go and read later,which will allow us to read the data which we want instead of reading the entire things.
	-- there are supported for all file based data structures.
csvFile.limit(10).write.mode("overwrite").partitionBy("DEST_COUNTRY_NAME").save("/tmp/partitioned-files.parquet")	
	-- This is will write a file on to the path with we have mentioned with 10 partition 
 ls /tmp/partitioned-files.parquet	
 	-- here we are ls on the file and we will see the file is written in multiple partitions with column name as below 
DEST_COUNTRY_NAME=Costa Rica/
DEST_COUNTRY_NAME=Egypt/
DEST_COUNTRY_NAME=Equatorial Guinea/
DEST_COUNTRY_NAME=Senegal/
DEST_COUNTRY_NAME=United States/	
Bucketing:
	-- Bucketing is another file organizing approach with which we can control the data that is specifically written on each file 
	-- This will avoid the shuflles laters when we go and read the file because the data with same bucketID will all be grouped together into one physical partition.
	-- This means data is preparationed according to how you expect to use the data later on which means we can avoid expensive shuffles when we are joining and aggregating.
val numberBuckets = 10
val columnToBucketBy = "count"
	-- Bucketing related
csvFile.write.format("parquet").mode("overwrite").bucketBy(numberBuckets, columnToBucketBy).saveAsTable("bucketedFiles")		
$ ls /user/hive/warehouse/bucketedfiles/
	-- This is gonna be saved in hive warehouse.
part-00000-tid-1020575097626332666-8....parquet
part-00000-tid-1020575097626332666-8....parquet
	-- Will look like this .
	-- Bucketing is supported only for spark manage tables.
Writting Complex types:
	-- Although all the complex types work well with spark we can thing like csv file is not that well with the complex type but where as parquet and ORC are.
Managing file sizes:
	-- Managing file sizes is important not only for writting the writting the data bit also reading it later on..
	-- When we are writting lot of small files spark will be having lot of metadata and that will be overwhlemed so spark is not good with small files. As we all know having a one large big file is also not suggestable because to read a few number of rows also we need the big fat block.
	-- Where here spark introduces a important way to control this file sizes automatically. As we can see if we write a file on to the disk we can see park automatically picks the number of partition even though we don't mention them.
	-- Not only that we can also take advantage of another tool inorder to limit our output file sizes so that we can generate a optimum file size.
MaxRecordsPerFile:
	-- We can use this file and limit the number of rows in the file and which will inorder result in the optimum sixe of the file.
df.write.option("maxRecordsPerFile", 5000),
	-- here if we say like above spark is gonna write only 5000 records in the file 


spark.sql("SELECT 1 + 1").show()
	-- We can do SQL in Spark and Python like above statement.
	-- This above statement returns a DataFrame and we can execute that programmatically just like other transformation and this also follows lazy evaluation.
	-- This is very powerful to spark because it is easy to write somethings on SQL than in Dataframes and API's
	-- We can execute multiline query quite simply by passing multiline string into the function 
spark.sql("""SELECT user_id, department, first_name FROM professors WHERE department IN(SELECT name FROM department WHERE created_date >= '2016-01-01')""")		
	-- We can simply do this ins Scala or python.
	-- The more power full thing is we can use SQL on Dataframes and Dataframe logic on SQL tables.
spark.read.json("Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/Data/flight-data/json/2015-summary.json").createOrReplaceTempView("some_sql_view") 
	-- by doing this or writting the above statemnet we are loading a file and creating a Dataframe and then immediately are creating a TempTable to acess the DataFrame in SQL way 
	-- Here we are spark format to read a Json file from a mention path.
spark.sql(""" SELECT DEST_COUNTRY_NAME, sum(count) FROM some_sql_view GROUP BY DEST_COUNTRY_NAME""").where("DEST_COUNTRY_NAME like 'S%'").where("`sum(count)` > 10").count() 	
	-- Here we are writting SQL query and making the changes to the data or the temptableview and then when we say count it is converted from SQL to DataFrame.
./sbin/start-thriftserver.sh
	-- This command will help us to run JDBC ODBC connection and this should be done in spark bin folder.
	-- This script accepts all bin/spark-submit commands-line options to see all available options for configuring this thrift server we need to run 
./sbin/start-thriftserver.sh --help
	-- When we do this by default the server listens to localhost 100000. We can also override this through environmental variable or system properties.
export HIVE_SERVER2_THRIFT_PORT=<listening-port>
export HIVE_SERVER2_THRIFT_BIND_HOST=<listening-host>
./sbin/start-thriftserver.sh \
--master <master-uri> \
...			
	-- For environment configartion We need to use the above thing 
/sbin/start-thriftserver.sh \
--hiveconf hive.server2.thrift.port=<listening-port> \
--hiveconf hive.server2.thrift.bind.host=<listening-host> \
--master <master-uri>
...
	-- For system properties use the above thing.
./bin/beeline
	-- We are kick starting the beeline.
beeline> !connect jdbc:hive2://localhost:10000
	-- We can test the connection using the above thing.
	-- When we do thi beeline will ask for Username and Password and we can simply type username on our machiner and blank password and this is for nin secure connection and for secure connection we need to go through other things.
Catalog:
	-- Highest level of abstraction in SparkSQL is in the catalog.The catalog is abstarction for the storage of metadata the data stored in our tables and as well as other helpful things like Database tables, function and views.
org.apache.spark.sql.catalog.Catalog 
	-- This catalogue is available in the above package and this also contains a number of helpful function for doing the things like databases functions tables and views.
CREATE TABLE flighties (DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)USING JSON OPTIONS (path '/home/santhoshi/test/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/json/2015-summary.json')
	-- Here we are just creating a table and then using "Using Json Options" we are loading the file on which we want to create a table.
	-- The Using syntax is important which we have used in the above example.If we don't mention the format spark will default to hive serde configaration.
	-- This SerDe configartion make cause problem to future user because spark SerDe' are much slower thant the of spark native serialization.
	-- When we want use hive we can simply use SAVE AS option to use it in hive.
CREATE TABLE flighties_csv (DEST_COUNTRY_NAME STRING,ORIGIN_COUNTRY_NAME STRING COMMENT "remember, the US will be most prevalent",count LONG)USING csv OPTIONS (header true, path '/home/santhoshi/test/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/csv/2015-summary.csv')						-- We can add comments to the tables which will help other developers to understand what the table exactly about
CREATE TABLE flights_from_select USING parquet AS SELECT * FROM flights;
	-- We can also create tables by using the normal commands.
CREATE TABLE partitioned_flighties USING parquet PARTITIONED BY (DEST_COUNTRY_NAME)AS SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 5	
	-- Here we are using partition by for creating a table.
CREATE EXTERNAL TABLE hive_flights (DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/home/santhoshi/test/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data-hive/'
	-- Here we are using hive to create a table in the SparkSQL.
CREATE EXTERNAL TABLE hive_flighties_2 ROW FORMAT DELIMITED FIELDS TERMINATED BY ','LOCATION '/home/santhoshi/test/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data-hive/' AS SELECT * FROM flights	
	-- Here we are creating a table in another simple way .
INSERT INTO flights_from_select SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count FROM flights LIMIT 20	
	-- Here we are inserting into tables.
INSERT INTO partitioned_flights PARTITION (DEST_COUNTRY_NAME="UNITED STATES")SELECT count, ORIGIN_COUNTRY_NAME FROM flightsWHERE DEST_COUNTRY_NAME='UNITED STATES' LIMIT 12	
	-- Another way if inserting into table.
DESCRIBE TABLE flights_csv
	-- Previous;y we have declared or wriiten some comments while creating the table and we will be able to view them in metadata using the above command.
SHOW PARTITIONS partitioned_flights
	-- We can see the partitioned schema for the data with the above commmand.
REFRESH table partitioned_flights
	-- Maintaining the tables metadata is important because that is when we will know if we are using the mostrecent set of data.
	-- There are 2 ways to refersh the tables metadata REFERESH TABLE refershes all cached entries associated with the table If the table was cached previously it is gonna be caches lazily next time it is cached.
MSCK REPAIR TABLE partitioned_flights
	-- Another way of refershing the table is using the REPAIR TABLE command which refreshes the partition maintained in the catalogue or the given table.
	-- This command focuses on collecting the new partition information an example might be writting the partition manually and the need of repair table accordinglly.
DROP TABLE flights_csv;		
	-- This command is used to drop the tables accordingly when not needed.When we drop a managed table both data and the table defination will be removed.
DROP TABLE IF EXISTS flights_csv;
	-- to check if the table exist and delete it we are gonna use the above command.
	-- When we are dropping the unmanaged table the data will not be dropped but the table will be and we will no longer be able to refer the data with the same table name.
CACHE TABLE flights
	-- We can cache the table like we cache the dataframe
UNCACHE TABLE FLIGHTS
	-- We can uncache them by using the above command.
CREATE VIEW just_usa_view AS SELECT * FROM flights WHERE dest_country_name = 'United States'							
	-- Here we are creating a view for the table flights 
CREATE TEMP VIEW just_usa_view_temp AS SELECT * FROM flights WHERE dest_country_name = 'United States'
	-- We can also create temporary view and this can be done for the cuurent session and will not be register to any of the databases
CREATE GLOBAL TEMP VIEW just_usa_global_view_temp AS SELECT * FROM flights WHERE dest_country_name = 'United States'	
	-- Here we are creating a global tempview of the table.This create a view for the entire spark application temporarily and will be lapsed when we end the session .
CREATE OR REPLACE TEMP VIEW just_usa_view_temp AS SELECT * FROM flights WHERE dest_country_name = 'United States'
	-- Here we are overwritting the view if already exist.
	-- We can overwrite both tempviews and regualr views.
SELECT * FROM just_usa_view_temp
	-- A view is effectively a tranfomation and spark will perform it at the time of querying.
val flights = spark.read.format("json").load("/data/flight-data/json/2015-summary.json")val just_usa_df = flights.where("dest_country_name = 'United States'")just_usa_df.selectExpr("*").explain	
	-- Here we are using the same view thing using dataframes and we can also say that the view is nothing but generating a dataframe from the existing dataframes.
EXPLAIN SELECT * FROM just_usa_view
	-- This will explain what happened on the view 
EXPLAIN SELECT * FROM flights WHERE dest_country_name = 'United States'
	-- We can say that the above both commands are same.		
DROP VIEW IF EXISTS just_usa_view;
	-- By using the above command we will be able to drop the views of a table.
SHOW DATABASES
	-- This will show all the databases present in the spark SQL
CREATE DATABASE some_db
	-- Here by doing this we are creating a Database.
USE some_db
	-- Here we are using the DB which we have created in the above statement.
SHOW tables
	-- When we do show tables on the databases it will show the tables if there are any in the database we are using.
SELECT * FROM flights 
	-- this will fail because we are in different databases and the flights which we are referring to will not be avaialble in the database which we have created
SELECT * FROM default.flights
	-- However to acess the tables whic are in the different databases can be referred from any other database.
SELECT current_database()
	-- this will say which database we will be using in the session.
USE default;
	-- We can switch back to default using the above command and switch back to the default DB.
DROP DATABASE IF EXISTS some_db;
	-- This will drop the Databse if exist 
SELECT [ALL|DISTINCT] named_expression[, named_expression, ...]
FROM relation[, relation, ...]
[lateral_view[, lateral_view, ...]]
[WHERE boolean_expression]
[aggregation [HAVING boolean_expression]]
[ORDER BY sort_expressions]
[CLUSTER BY expressions]
[DISTRIBUTE BY expressions]
[SORT BY sort_expressions]
[WINDOW named_window[, WINDOW named_window, ...]]
[LIMIT num_rows]
named_expression:
: expression [AS alias]
relation:
| join_relation
| (table_name|query|relation) [sample] [AS alias]
: VALUES (expressions)[, (expressions), ...]
[AS (column_name[, column_name, ...])]
expressions:
: expression[, expression, ...]
sort_expressions:
: expression [ASC|DESC][, expression [ASC|DESC], ...]
	-- This are all the select statements and the layouts.
SELECT CASE WHEN DEST_COUNTRY_NAME = 'UNITED STATES' THEN 1 WHEN DEST_COUNTRY_NAME = 'Egypt' THEN 0 ELSE -1 ENDFROM partitioned_flights	
	-- we are using the case statemnet thats all 
CREATE VIEW IF NOT EXISTS nested_data AS SELECT (DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME) as country, count FROM flights				
	-- this is how we create a nested view of a table or a data set.
SELECT * FROM nested_data
	-- We can query it and see what it look like 
SELECT country.DEST_COUNTRY_NAME, count FROM nested_data	
	-- We can also use individual columns in struct in order to see how it looks like but the only thing we need to use is (.)	dot syntax.
SELECT country.*, count FROM nested_data
	-- If we like we can also select all the subvalues from the struct by using the struct name and select all the sub columns although they aren't subcolumns but doing that way makes them doing anything them like we do to columns.
SELECT country.*, count FROM nested_data
	-- Like this 
SELECT DEST_COUNTRY_NAME as new_name, collect_list(count) as flight_counts, collect_set(ORIGIN_COUNTRY_NAME) as origin_set FROM flights GROUP BY DEST_COUNTRY_NAME	
	-- Here we are trying to create a list 
SELECT DEST_COUNTRY_NAME, ARRAY(1, 2, 3) FROM flights
	-- We can also create an array with in a columns.
SELECT DEST_COUNTRY_NAME as new_name, collect_list(count)[0] FROM flights GROUP BY DEST_COUNTRY_NAME
	-- We also query the list by using positions by using python like array query syntax.
CREATE OR REPLACE TEMP VIEW flights_agg ASSELECT DEST_COUNTRY_NAME, collect_list(count) as collected_countsFROM flights GROUP BY DEST_COUNTRY_NAME	
	-- we have created an array so that we can use explode function the same thing
SELECT explode(collected_counts), DEST_COUNTRY_NAME FROM flights_agg
	-- Now we are using the explode function and exploding 
	-- Like in above statement we will also be able to convert the arrays into normal rows We can do this by using explode function 	
SHOW FUNCTIONS
	-- This will show all the function in the spark SQL. we can see list of all function in SQL.
SHOW SYSTEM FUNCTIONS
	-- By this we are more specifically saying spark which type of function system function or user function of type.
SHOW USER FUNCTIONS
	-- Here we are doing the exact opposite of above function. and here it will show all the default user functions.
SHOW FUNCTIONS "s*";
	-- If we want to see what are the functions that are all with S* we need use this function 
SHOW FUNCTIONS LIKE "collect*";
	-- We can also include like keyword even though it is not nessacary.
DESCRIBE:
	-- using the describe keyword we can know more about the function which we have see in the list 	
def power3(number:Double):Double = number * number * number
spark.udf.register("power3", power3(_:Double):Double)
	-- The above is user defined function and we used it any where we want .
SELECT count, power3(count) FROM flights
	-- This is gonna resuly differently and according to the user defined function.
	-- You can also register functions through the Hive CREATE TEMPORARY FUNCTION syntax
SELECT dest_country_name FROM flights GROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5
	-- Here we are writting a normal query and that will give the below output.
Output:
 +-----------------+
|dest_country_name |
+-----------------+
| United states 	|
| Canada 			|
| Mexico			|
| United Kingdom	|
| Japan				|
+-----------------+			

SELECT * FROM flights WHERE origin_country_name IN (SELECT dest_country_name FROM flightsGROUP BY dest_country_name ORDER BY sum(count) DESC LIMIT 5)		-- Here we are are placing the subquery as filter and check if the orginal country exist in the list.
	-- This is query is uncorrelated because we didn;t use the output from outer scope.
UDF:
val add_n = udf((x: Integer, y: Integer) => x + y)
myRange.withColumn("unique_id",add_n(lit(1), col("number").cast("int")))
	-- This is an UDF and here we are defining how to add a new column with number i.e. continous number so that we cab get the numbers assigned to all the rows in the data frame
SET spark.sql.shuffle.partitions=20
	-- Here we are satting configuaration of SQL not in detail
---------------------------------------

case class Flight(DEST_COUNTRY_NAME: String,ORIGIN_COUNTRY_NAME: String, count: BigInt)
	-- Here we are defining a case class.
val flightsDF = spark.read.parquet("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/Data/flight-data/parquet/2010-summary.parquet/")	
	-- We will read a file and that is still a dataframe but we can need to change it like we want.
flightsDF.printSchema()
root
 |-- DEST_COUNTRY_NAME: string (nullable = true)
 |-- ORIGIN_COUNTRY_NAME: string (nullable = true)
 |-- count: long (nullable = true)
 	-- the above is the way which spark is reading a Dataframe on it own by taking its own types and we defined our owm in case class and we are gonns use that to chnage the types in the Dataframe that we have created	
val flights = flightsDF.as[Flight]
	-- Now we are changing the Dataframe which we have created by loading which is flightsDF to case class flight 
	-- when we see in the above schema spark has considered the above count as long but we have chnaged that to bigInt by using case class
flights.show(2)
	-- Actions we use are same for dataframes and datasets
flights.first.DEST_COUNTRY_NAME	output:res47: String = United States 
	--we don’t need to do any type coercion, we simply specify the named attribute of the case class and get back, not just the expected value but the expected type, as well:
Transformations for Datasets:
	-- Transformation for Datsets are same as those of which we saw for dataframes.Any transformation that we would read in this section is valid on a dataset , and we encourage you to look through the specific sections on relevant aggregations or joins.
	-- In addition to those transformation datasets alsi allow us to specify more complex and strongly typed transformations that we could perform of Dataframes alone because we manipulate Raw JVM types. 
	-- To get grip of the raw object manipulation we will be doing a filter operation on the data type we created.
Filtering:
	-- We will be doing a simple example in which we will be creating a simple function that accepts a flight and simply returns a boolean value that described the origin of the flight and destination are same This is not a UDF but a generic function.
	-- Now we will be defining a function to achive the filter which we want or the above which we are trying to acheive.
	-- by specifiying the function we are asking or forcing spark to evaluate this function on every row in the dataset.This can be very resource intensive for a simple filter operation it is always preferred to write a SQL expression. This will greatly help us to reducing the cost of filtering out the data and also allowing us to manipulate the dataframe which we have filter to a dataset later on 
def originIsDestination(flight_row: Flight): Boolean = {
return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME
}	
	-- Here we are defining a function to apply the filter which we want in which the filghts destination country name and orgin country name are same and should return a boolean value to check that.
	-- Now we can pass this function to the filter method specifying the each row it should verify that this function return true and this process will filter out the data accordingly.
flights.filter(flight_row => originIsDestination(flight_row)).first()
	-- here we using the dataset we have created and appliying a filter and saying that use this function in the filter and we said first so it shows the first one.
output:
Flight = Flight(United States,United States,348113)
	-- It shows the matching and return a booelan value as defined.
	-- As we saw earlier, this function does not need to execute in Spark code at all. Similar to our UDFs, we can use it and test it on data on our local machines before using it within Spark.
flights.collect().filter(flight_row => originIsDestination(flight_row))
	-- As this dataset is small enough to collect to a driver on which we can perform same collect operation and filter as well.
output:
	Array[Flight] = Array(Flight(United States,United States,348113))
Mapping:
	-- Filtering is a simple operation but sometimes we need to map one value to another. We did this with our function in previous example: it accepts a filgth and return a boolean value but other times we might actually need to perform something more sophisticates as Extract value compare a set of values or something similar 
	-- The simplest example n maipulation our Dataset such that we can extract on value from each row This effectively performing the dataframe like select on our dataset.
	-- Extracting the destination.
val destinations = flights.map(f => f.DEST_COUNTRY_NAME)
	-- when we do the above we end up with a Dataset of type string. This is because spark already knows the JVM type that this sult should return and allows us to benfit from compile time checking if for some reason it is invalid.We can collect this and get back an array of strings on the driver:
val localDestinations = destinations.take(5)
	-- now we are taking the 5 from the destination that we have extracted from the entire dataset and we all know this is not possible on Dataframes.
	if we do so we can achieve the same with dataframes and it is also recommended to do that but if you want to use this case then there should be something related to column by column maniplutaion to do so.
Joins:
	-- they are similar to that we have done with dataframes also datasets provide more sophisticates way that is joinWith methods. joinWith is almost similar to co-group with on RDD's and we basically end up with 2 nested datasets in "1".
	-- Each column represents on Dataset and they can be manipulated accordingly.
	-- This can be useful when we need to maintain more information in the join or perform somemore sophisticated anipulation on the entire result set like an advance map or filter.
	-- Creating a fake flight metadata dataset to demonstrate joinWith:					
case class FlightMetadata(count: BigInt, randomData: BigInt)
	-- function and we are defining that with a case class and we are saying count is bigInt and randomData bigInt.
val flightsMeta = spark.range(500).map(x => (x, scala.util.Random.nextLong)).withColumnRenamed("_1", "count").withColumnRenamed("_2", "randomData").as[FlightMetadata]	
	-- here we are using spark range function and mentioning the range as 500 and saying .map x=> x , random long and with column renames as _1 as count and _2 as randomData
val flights2 = flights.joinWith(flightsMeta, flights.col("count") === flightsMeta.col("count"))	
	-- here we are using joining flights with flightsmeta where flights.count column is equal to flightsMeta.coount column.
	-- when we do this we end up with a dataset of type key value pairs in which each row represents flight and flight metadata we can query this as dataset or dataframe of type complex
flights2.selectExpr("_1.DEST_COUNTRY_NAME")
	-- We are acessing it as dataframe and as complex type too.
flights2.take(2)
	-- We can collect them as we did before.
val flights2 = flights.join(flightsMeta, Seq("count"))
	-- normal join is alos possible but we will be ending up loosing the type data in JVM	
	-- We always define another dataset to gain it back It is important to know that there is no problem in joining a dataset or dataframee they both result in the same thinge.
Grouping and aggregations:
	-- grouping and aggregation follow the samw fundamentals as we discussed previously for dataframe.
	-- So roll by groupy by cube will still aplly but they are gonna result in a dataframe and we will lose the information type.
flights.groupBy("DEST_COUNTRY_NAME").count()
	-- This is often is not too big to deal but if we want to keep the type information clear there aggregation which are different can be performed and an example for that would be groupByKey method.
	-- This groupByKey will help us to group by specific key in th dataset nd get a typed dataset in return. This function however will not accept a column name but rather a function.This makes it possible to specify more sophisticated grouping functions that are more kin to something like below:
flights.groupByKey(x => x.DEST_COUNTRY_NAME).count()
	-- although we are able perform an aggregation on dataset and getting result dataset which is also a dataset with types not changed that is not a good thing because now we are introducing JVM types as well as a function for which spark cannot do any optimisatiom so we will be seeing a performance difference and we can see this when we inspect the explain plan of dataset.
flights.groupByKey(x => x.DEST_COUNTRY_NAME).count().explain
	-- Here we are effectively appending a new column to dataframe with a function and then performing a grouping on that
Output:
== Physical Plan ==
*HashAggregate(keys=[value#1396], functions=[count(1)])
+- Exchange hashpartitioning(value#1396, 200)
+- *HashAggregate(keys=[value#1396], functions=[partial_count(1)])
+- *Project [value#1396]
+- AppendColumns <function1>, newInstance(class ...
[staticinvoke(class org.apache.spark.unsafe.types.UTF8String, ...
+- *FileScan parquet [D..
	-- After we perform grouping on a dataset using key we can operate on key,value dataset with functions that will manipulate grouping as raw objects:
def grpSum(countryName:String, values: Iterator[Flight]) = {
values.dropWhile(_.count < 5).map(x => (countryName, x))
}
	-- Here we are defining another function 									
flights.groupByKey(x => x.DEST_COUNTRY_NAME).flatMapGroups(grpSum).show(5)
	-- passing the same into a dataset to generate our required or desired output.
Output:
 	| _1| _2|
+--------+--------------------+
|Anguilla|[Anguilla,United ...|
|Paraguay|[Paraguay,United ...|
| Russia|[Russia,United St...|
| Senegal|[Senegal,United S...|
| Sweden|[Sweden,United St...|
+--------+--------------------+			
def grpSum2(f:Flight):Integer = {
1
}
	-- Defining another function 
flights.groupByKey(x => x.DEST_COUNTRY_NAME).mapValues(grpSum2).count().take(5)
	-- Passing it through a dataset.
	-- We can also create a new manipulation and define how group should be reduced.
def sum2(left:Flight, right:Flight) = {
Flight(left.DEST_COUNTRY_NAME, null, left.count + right.count)
}
	-- Defining the function to reduce the group 
flights.groupByKey(x => x.DEST_COUNTRY_NAME).reduceGroups((l, r) => sum2(l, r)).take(5)					
	-- Here we are passing it into a dataset as we defined a function to achive a groupBykey operation in more effective way 
	-- we should also understand that this is more expensive process than aggregation immediately after scanning especially because it ends up in the same end result 
flights.groupBy("DEST_COUNTRY_NAME").count().explain
== Physical Plan ==
*HashAggregate(keys=[DEST_COUNTRY_NAME#1308], functions=[count(1)])
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#1308, 200)
+- *HashAggregate(keys=[DEST_COUNTRY_NAME#1308], functions=[partial_count(1)])
+- *FileScan parquet [DEST_COUNTRY_NAME#1308] Batched: tru...
	-- this will be the explain plan  and this also should motivate using datasets nly with user defined encoding surgically and only where is makes sense.
	-- This might be the beggining of a big datapipeline or at end of the one.
---------------------------

spark.sparkContext
	-- We can acess spark context via above call. 
Interoperating between Dataframes,Datasets and RDD's:
	-- One of the easiest way to get RDD's is to get it from Datasets or Dataframes.We will see that when we are converting from DataSet[t] to an RDD we will the appropriate native type T back
spark.range(500).rdd
	-- Converts a Dataset[long] to an RDD[long]
spark.range(10).rdd.toDF()
	-- Creating a Dataframe from a RDD.
val myCollection = "Spark The Definitive Guide : Big Data Processing Made Simple".split(" ")	
	-- here we are assigning a line to a val myCollection and then splitting the line with tab space.
val words = spark.sparkContext.parallelize(myCollection, 2)
	-- Here we are parallelizing myCollection and saving the same on words.
	-- To create a RDD for collection we will need to use parallelize method on sparkContext with in a spark session.
	-- This turns a single node collection into a parallel collection. When creating a parallel collection we can specify the number of partition into which we would like to distribute the array In the above case we have created 2 partitions.
words.setName("myWords")
	-- An additional feature is that we can then name a RDD to show up in spark UI according to given name.
words.name
	-- When we do this it is gonna show the name which we have assigned to that RDD.
spark.sparkContext.textFile("Users/kartikeyan/Desktop/textfilekari")
	-- We are just reading textfile as RDD. and this RDD contains each line which file we have loaded as eas record in the RDD.
	-- Alternatively we can also read the data in a way that each textfile would become a single record.
	-- The use case here would be where each file is a file which consist of a large Json object or some document that we will opreate as an individual.
spark.sparkContext.wholeTextFiles("/some/path/withTextFiles")
	-- This command will read all the text file which are present in that path and will considere the whole one text file as an record.
	-- In this type of RDD's name of the file would be the first object and all the thing which are in the file would be second string.
words.distinct().count()
	-- Distinct method call will remove duplicates in a RDD.
def startsWithS(individual:String) = {
individual.startsWith("S")
}
	-- Here we are defining a function to filter out things starts with "S"
words.filter(word => startsWithS(word)).collect()
	-- We are using that function in filter.	
	-- Filter is somthing which is very similar to that fo the where clause in SQl. We can look through the RDD and see which matche sour predicate function.
	-- The important thing is that we need to make sure that the function returns a booelan value so that we can pass through the filter and the input would be whatever the row in the RDD which we are passing.
	-- In the above example we are filtering out the words which are starting with "S"
	-- In the result of the filter and function together its gives spark and simple.
val words2 = words.map(word => (word, word(0), word.startsWith("S")))
	-- Mapping again is the same operation which we have did for the Datasets it is similar.
	-- we specify a function that return the value that we want given the correct input and then we apply that record by record
	-- Similar to the above example we are mapping word to word with which the word starts with "S"
	-- By this we will be creating a tuple which have 3 things (word,starting of the word letter, boolean value)
	-- So we will be having something similar to this:
Array[(String, Char, Boolean)] = Array((Spark,S,true), (The,T,false), (Definitive,D,false), (Guide,G,false), (:,:,false), (Big,B,false), (Data,D,false), (Processing,P,false), (Made,M,false), (Simple,S,true))	
val bany = words2.filter(record => record._3).take(5)
	-- so as we all know to take out the boolean values which we want we need to use filter and we here we will be only filter based on the boolean which is present in the 3rd place of the tuple
	-- suppose for example we have a tuple  (Simple,S,true) simple that is the first place of the tuple is represented as _1 and s the second place of the tuple will be represented as _2 and third which is boolean value in the tuple will be represented as _3 and we will only be able to refer the elements in tuple by saying _1 ,_2,_3
	-- when we are saying record => record_3 we are refereing the booelan value in that tuple and that would be _3 and filtering out the all remaning tuples bases on the values of 3 
	-- When we saw the result in the above Words2 we see that entire thing is only having 2 tuples which are having to values and when we apply a filter that is only gonna return the true values.
	-- Simple we can say filter is something which will give the output which will true as boolean value
bany.collect
	-- as we assigned the variable to the filtered out values that will have  Array[(String, Char, Boolean)] = Array((Spark,S,true), (Simple,S,true))
words.flatMap(_.toSeq).take(5)
words.flatMap(word => word.toSeq).take(5)
	-- The above both word => word and _ does the same thing.
	-- So flat map is extension to the map function which we have used above.sometimes each current row should return multiple rows instead. If we have group of words and we want to split the every word into a single letter (or) a charcter wr don't need to manually manpulate it flatmap does that for us.
Output:
	Array[Char] = Array(S, p, a, r, k, T, h, e, D, e)
words.sortBy(word => word.length() * -1).take(2)
words.sortBy(_.length() * -1).take(2)	
	-- Here we are sorting the RDD with the help of sort method and any sortings in RDD will be done in sorting method.
	-- So in the above example we are iterating through every word which is present in the word and then sorting the each word with the lenght of the word 
Output:
	Array[String] = Array(Definitive, Processing, Simple, Spark, Guide, Data, Made, The, Big, :)
val fiftyFiftySplit = words.randomSplit(Array[Double](0.5, 0.5))
	-- We can also randomly split an RDD into an Array of RDDs by using the randomSplit method, which accepts an Array of weights and a random seed
Actions: 
	-- We know what are actions and we ahve used them with Datasets and Dataframes and we know if we want to kick off our transformation done on RDD we take an action.
spark.sparkContext.parallelize(1 to 20).reduce(_ + _) // 210
	-- here we are using Reduce and with reduce we can reduce any kind of RDD into one value 
	-- In this example first we are parallelizing the collection and then we are using a reduce function on the parallelize and in that reduce we are adding the every value in the collection which we just parlellised.
def wordLengthReducer(leftWord:String, rightWord:String): String = {
if (leftWord.length > rightWord.length)
return leftWord
else
return rightWord
}
	-- Here we are wriiting a function and declaring 2 variable leftword and rightwords and both of them as string and we are also defining that this gonna return a string
	-- Here we are also wriiting a if fucntion and comparing the left and right variable and saying if leftword.length greater than the right word then return left word or else right.
	-- 	wordLengthReducer is name of the function and we will be refering the passing the function through any method referring to this as the name of the function 
	-- So we will be doing this on Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)
words.reduce(wordLengthReducer)
	-- we are passing the function which we have defined in the reduce method of the RDD.
	-- When we do this it is gonna return the highest lenght word 
Output:
res25: String = Definitive
words.count()
	-- This will count the number of row in a RDD.
val confidence = 0.95
val timeoutMilliseconds = 400
	-- Here we are giving some values to these varaible so that we can use them some where else.
words.countApprox(timeoutMilliseconds, confidence)
	-- Here we are using the count approx function and then we are passing the varaible which we have decalred through the countApprox.
	-- Even though when we say that countApprox it is weird but the method is very sophsticated.
	-- This is nothing but the approximation of the count method which we have just used but this must be executed with in the timeout and result inaccurate count which the time is out.
	-- The confidence in this the probability the error bounds of the result will contain a true value.that is countApprox is called repeatedly with confidence 0.9 e would expect the 90% of result contain the true count. Th confidence must be in range 0-1 or else and exception will be thrown.
words.countApproxDistinct(0.05)
	-- There are two implementations of this, both based on streamlib’s implementation of “HyperLogLog in
Practice: Algorithmic Engineering of a State-of-the-Art Cardinality Estimation Algorithm.” In the first implementation, the argument we pass into the function is the relative accuracy. Smallervalues create counters that require more space. The value must be greater than 0.000017:			
	-- With the other implementation you have a bit more control; you specify the relative accuracy based on two parameters: one for “regular” data and another for a sparse representation.
	-- The two arguments are p and sp where p is precision and sp is sparse precision. The relative accuracy is approximately 1.054 / sqrt(2 ). Setting a nonzero (sp > p) can reduce the memory consumption and increase accuracy when the cardinality is small. Both values are integers:å	
words.countApproxDistinct(4, 10)
words.countByValue()
	-- The method counts the number of values in the given RDD.However this will be done by loading the result set into the memory of the driver. We should use theis method onlt when we know the number of rows are low and there are only little distinct value because ot load everything into the memory.
words.countByValueApprox(1000, 0.95)
	-- This does the same thing as the previous function, but it does so as an approximation. This must execute within the specified timeout (first parameter) (and can return incomplete results if it exceeds the timeout).			
	-- The confidence is the probability that the error bounds of the result will contain the true value. Thatis, if countApprox were called repeatedly with confidence 0.9, we would expect 90% of the results to contain the true count. The confidence must be in the range [0,1], or an exception will be thrown
words.first()
	-- Will returns the first value in the Dataset.
spark.sparkContext.parallelize(1 to 20).max()
spark.sparkContext.parallelize(1 to 20).min()
	-- Will return the max and min values of the dataset and in the above we can say collection which is parallelized.
words.take(5)
	-- take and its derivative method takes number of value in the RDD.
	-- This works by first scanning on the partition and then using the result for the partition to estimate the number of additional partition which need to satisfy the limit
words.takeOrdered(5)
	-- opposit to top
words.top(5)
	-- Top 5 will take the top 5 rows of the RDD
val withReplacement = true
val numberToTake = 6
val randomSeed = 100L
 	-- Declaring varaiable assigned with values and that is possibily used in the next steps to pass through something
words.takeSample(withReplacement, numberToTake, randomSeed)	
	-- We can use this takeSample to specify the a fixed random sample from the RDD.
	-- We should specify weather this should be done with replacement the numbe of values as well as the random speed 
words.saveAsTextFile("file:/tmp/bookTitle")
	-- We can also save the dataframe which we have created into a textfile and in the mentioned path.
	-- With Rdds we will not be able to save to a data source in the conventinal sense. we must iterate to partition inorder to save the content of each partition to an external database.
	-- This is lowlwvwl approach that reveal the underlying operation that is being performed in Struvctured API's.Spark will take each partition and write that out to a destination.
import org.apache.hadoop.io.compress.BZip2Codec
	 -- Here we are importing a statement which will have the function or something which we want to implement
words.saveAsTextFile("file:/tmp/bookTitleCompressed", classOf[BZip2Codec])
	-- Inorder to implement a commpression codec in RDD we have to import it from Hadoop.
words.saveAsObjectFile("/tmp/my/sequenceFilePath")
	-- Here we are saving words as a sequential file.
	-- Spark orginally grow out of hadoop ecosystem.so it has a fairly tight integrating with hadoop system. A sequence is a flat file which consist of binary keys value pairs Its is extensively used in Mapreduce as input/output formats.
	-- Spark can write to sequenceFiles using the saveAsObjectFile method or by explicitly writing key–value pairs which we will see later on.
words.cache()
	-- We can either cache or persist a RDD. 
	-- We can name it if we use the setName function that we referenced previously in the examples.
org.apache.spark.storage.StorageLevel
	-- We are importing this statement to implement the desired function.
	-- We can specify a storage level as any of the storage levels in the singleton object:
	-- which are combinations of memory only; disk only and separately, off heap.		
words.getStorageLevel
	-- This will return the storage level we are in 
spark.sparkContext.setCheckpointDir("/some/path/for/checkpointing")
words.checkpoint()
	-- checkpointing the is the one feature that is not available in Dataframes.
	-- Checkpointing is saving the RDD to the disk so the future reference to this RDD point to those intermediate partition on the disk rather recomputing the RDD from the original source.
	-- this similar to cahing but the only thing this will not be stored on the memory instead it will be stored on the	disk.This can be helpful when performing iterative computation, similar to the use cases for caching:
	-- Now, when we reference this RDD, it will derive from the checkpoint instead of the source data. This can be a helpful optimization.
words.pipe("wc -l").collect()
	-- The pipe method is one of the sparks more intersting method.with pipe you can return an RDD created by piping elements to forked external process.
	-- The resulting RDD is computed by xecuting the given process once per partition. all elements of each input partition are written to a process's stdin as line of input sperated by a new line of stdoutThe resulting partition consists of theprocess’s stdout output, with each line of stdout resulting in one element of the output partition. Aprocess is invoked even for empty partitions.The print behavior can be customized by providing two functions.We can use a simple example and pipe each partition to the command wc. Each row will be passed inas a new line, so if we perform a line count, we will get the number of lines, one per partition.
words.map(word => (word.toLowerCase, 1))
	-- Here we are using same RDD words and then using iteraton over the RDD and are converting it to lowercase.
val keyword = words.keyBy(word => word.toLowerCase.toSeq(0).toString)
	-- In the previous example we saw how to create a key. However we can use the same keyBy function to acheive the same result by specifying a function that creates a key from our current value.
	-- In this example we are keying with first letter in the word Spark then keeps the value as the complete record in the RDD.
	-- So here we are just explaning or specifing what spark should consider as key and once we specify that the spark will do the remaining by assigning the actual records from which we have picked key and assign them as values.	
keyword.mapValues(word => word.toUpperCase).collect()
	-- now that we have key value pairs we should begin to manipulate the pairRDD's.If we have a tuple spark will assume the first element as key and second as value. So in this format we can explicitly choose to map over values and ingnore the individual keys 
	-- In the above Example we should understand that we are changing the values to lowercase by using a function called as Mapvalues and only values are gonna change
	-- One more important thing to understand is that if we have only 2 element in a tuple spark will automatically consider this as key value pair RDD.
Output:
Array[(String, String)] = Array((s,SPARK), (t,THE), (d,DEFINITIVE), (g,GUIDE), (:,:), (b,BIG), (d,DATA), (p,PROCESSING), (m,MADE), (s,SIMPLE))			
	-- Values are converted to UpperCase.
keyword.flatMapValues(word => word.toUpperCase).collect()
	-- Here we are performing falt map on only the values of the tuple or pairRDD according to spark.We can flat map over the rows as we saw previously .
Output:
Array[(String, Char)] = Array((s,S), (s,P), (s,A), (s,R), (s,K), (t,T), (t,H), (t,E), (d,D), (d,E), (d,F), (d,I), (d,N), (d,I), (d,T), (d,I), (d,V), (d,E), (g,G), (g,U), (g,I), (g,D), (g,E), (:,:), (b,B), (b,I), (b,G), (d,D), (d,A), (d,T), (d,A), (p,P), (p,R), (p,O), (p,C), (p,E), (p,S), (p,S), (p,I), (p,N), (p,G), (m,M), (m,A), (m,D), (m,E), (s,S), (s,I), (s,M), (s,P), (s,L), (s,E))		
keyword.keys.collect()
keyword.values.collect()
	-- We can simply call the above as extracting key value pairs. By using above methods we can extract keys and values seperately.
Output:
Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)
keyword.lookup("s")
	-- One intersting task we can be able to on RDD is to lookup the result for particulat key.we lookup “s”, we are going to get both values associated with that—“Spark” and “Simple” 		
val distinctChars = words.flatMap(word => word.toLowerCase.toSeq).distinct.collect()
	-- Here we are using a RDD and then doing a flatMap on the RDD and then changing everything into lower case and then doing a distnct on lowercased RDD and then performing a collect and then assigning that to a variable which we may use later on  
import scala.util.Random
	-- Here we are using a import statemnet to implement the Random function.
val sampleMap = distinctChars.map(c => (c, new Random().nextDouble())).toMap
	-- Then we are randoming it 
words.map(word => (word.toLowerCase.toSeq(0), word)).sampleByKey(true, sampleMap, 6L).collect()
	-- Then are sampling it.
	-- There are 2 ways to sample the RDD wuth set of keys. We can do it via approximation or exactly.
	-- Both operations can do so with or without replacement as well as sampling by a fraction by a given key Thsis is done via simple random sampling which is done with one pass over RDD.
	-- which produces a sample of size that’s approximately equal to the sum of math.ceil(numItems * samplingRate) over all key values:
words.map(word => (word.toLowerCase.toSeq(0), word)).sampleByKeyExact(true, sampleMap, 6L).collect()
	-- This method differs from sampleByKey in that you make additional passes over the RDD to create a sample size that’s exactly equal to the sum of math.ceil(numItems * samplingRate) over all key values with a 99.99% confidence.
	-- When sampling without replacement, you need one additional pass over the RDD to guarantee sample size; when sampling with replacement, you need two additional passes.
val chars = words.flatMap(word => word.toLowerCase.toSeq)
	-- Here we are doing the flat map on word and then we are converting them in to lowercase and toSeq
val KVcharacters = chars.map(letter => (letter, 1))
	-- Then we are converting the above letters which we have generated by flatmpping and assigning them to a varaible and then assigning the every letter with "1" so now this is gonna become tuple or simply we can say key value pairs
def maxFunc(left:Int, right:Int) = math.max(left, right)
	-- then we are defining a function 
def addFunc(left:Int, right:Int) = left + right
	-- Then we are defining a function
val nums = sc.parallelize(1 to 30, 5)
	-- Same old story
val timeout = 1000L //milliseconds
	-- We are assigning this to a varaible
val confidence = 0.95
	-- assginging this to varaible
KVcharacters.countByKey()
	-- Then we are doing a count by key operation on the above thing we have generated
KVcharacters.countByKeyApprox(timeout, confidence)
	-- This is we are doing a approximation.
KVcharacters.groupByKey().map(row => (row._1, row._2.reduce(addFunc))).collect()
	-- Here we are doing a groupBy key opeartion and see how it works.
	-- groupByKey followed by a map is a wrong way to approach a problem. The fundamental issue here is that each executor must hold all values for given key in the memory before applying the function to them. Okay then why is this problematic?
	-- If we have a massive key skew some partitions might be completely overloaded with aton of values for a given key and we will get out of memory errors.
	-- This abviously will not have problem with current dataset but when we are working on a bigger scale and this is not compulsory to happen but there is a chance of this happening.
	-- There are usescase where groupByKey will make sense. If we know the value size of the key and also to know they will fit into the given executor memory.
	-- It is preferred to know all this thing before we gonna do anything with reduceBy and groupBy.
KVcharacters.reduceByKey(addFunc).collect()KVcharacters.reduceByKey(addFunc).collect()
	-- Because we are performing a simple count a much more stable approach is to perfrom flatmap and then perform a map to map each letter instance to the number one and then perform reduceByKey with a summation function in order to collect back the Array.
	-- This implementation is much more stable because reduce happens with in the each partition and doesn't need to put everything inside the memory additionally there is no incurred shuffle during this operation everything happens at each worker individually before performing a final reduce.
	-- This enchances the speed at which we perform a operation as well as stability of the operation	
Output:
	 Array[(Char, Int)] = Array((d,4), (p,3), (t,3), (b,1), (h,1), (n,2), (f,1), (v,1), (:,1), (r,2), (l,1), (s,4), (e,7), (a,4), (i,7), (k,1), (u,1), (o,1), (g,3), (m,2), (c,1))
	 -- This reduceByKey will return RDD of a group and sequence of elements that are not guarnteed to have an order. This will make sense only when we don't want any order is want a normal order then this inappropriate.
nums.aggregate(0)(maxFunc, addFunc)
	-- Another function is aggregate. This function requires a null and a start value and then requires us to mention 2 different functions.
	-- The first aggregate with in the partitions and the second aggregates across the partitions. The start value will be used at both aggregation levels
	-- aggregation does have some performance issue because this performs final aggregation on driver.If the output of the executor is high then they take down the driver with out of memoery exception.
	-- There is another thing called as tree aggregation this does the same thing as aggregation but does it in a different way. So this treeAggregate pushesdown some of sub aggregations before performing the final aggregation.Having multiple level will help us from getting the outofmemory exception.
	-- This tree base thing are certianly introduced to get stability with th operations.
val depth = 3
nums.treeAggregate(0)(maxFunc, addFunc, depth)
	-- this is how we write the treeAggregate.
KVcharacters.aggregateByKey(0)(addFunc, maxFunc).collect()
	-- Here we are using aggregate by key. Instead of doing it by partition by partition it does based on the key.
	-- The start value is similar to that of the aggregate and treeAggregate.			 
val valToCombiner = (value:Int) => List(value)
val mergeValuesFunc = (vals:List[Int], valToAppend:Int) => valToAppend :: vals
val mergeCombinerFunc = (vals1:List[Int], vals2:List[Int]) => vals1 ::: vals2
	-- Here we are assigning everything to val's
val outputPartitions = 6
	-- We are setting the output partitions as 6.
KVcharacters.combineByKey(valToCombiner,mergeValuesFunc,mergeCombinerFunc,outputPartitions).collect()		 
 	-- Here we are performing combineByKey operation.
 	-- Combiner operated on given key and merges the value according to function It then goes an merges different output from combiners and gives us the result.
 	-- We can set the number of partitioner number as custome partitioner as well.
KVcharacters.foldByKey(0)(addFunc).collect()
 	-- foldBykey merges the value foreach key using a associative function and a neutral zerovalue. Which can be added to result the arbitary number of times and must not change the results 
import scala.util.Random
	-- Here we are importing a statment to implement that function.
val distinctChars = words.flatMap(word => word.toLowerCase.toSeq).distinct
	--We are doing the same thing 
val charRDD = distinctChars.map(c => (c, new Random().nextDouble()))
val charRDD2 = distinctChars.map(c => (c, new Random().nextDouble()))
val charRDD3 = distinctChars.map(c => (c, new Random().nextDouble()))
	-- Out of that we are creating 3 different RDD's
charRDD.cogroup(charRDD2, charRDD3).take(5)		 
	-- Cogroup gives us ability to group 3 pairedRDD's in scala and 2 in python.This joins the given value by key.this is in simple is a grouped bases join on RDD's
	-- While doing this we can set number of output partitions or a custompartitioning function to control exactly how this data is distributed accross the cluster.
	-- The result with group with key on oneside and relvant key on the other side.
val keyedChars = distinctChars.map(c => (c, new Random().nextDouble()))
	-- Here we are doing the same onld thing
val outputPartitions = 10
	-- Here we are metining the number of output patitioners
KVcharacters.join(keyedChars).count()
	-- Here are peforming an inner join on RDD's
KVcharacters.join(keyedChars, outputPartitions).count()		
	-- Here we are also mentionong the number of output partioners and in this case they would be 10
val numRange = sc.parallelize(0 to 9, 2)
	-- here we are taking a collection and parllelizing that and the mentioning the partitions as 2 and then assigning it to a varaible so that we can use it
words.zip(numRange).collect()	
	-- Here we are taking 2 dataframes and performing a ZIP on both of them.Zips alloes us to ZIP 2 RDD's where the lenght and the number of partitions are same.
	-- This creates a PairRDD which will have same number of elements as well as same numbe of partitions.
Output:
[('Spark', 0),
('The', 1),
('Definitive', 2),
('Guide', 3),
(':', 4),
('Big', 5),
('Data', 6),
('Processing', 7),
('Made', 8),
('Simple', 9)]		
Controlling Partitions:
	-- with RDD's we can control exactly and physically how the data is distributed across the cluster.Theese are almost similar to structured API's but here key comes into the picture.
	-- and additionally ability to specify the partitions.
words.coalesce(1).getNumPartitions 
	-- Coalesce effectively collapses the partition on the one executor which creating shuffle
	-- For instance lets consider that Our RDD is currently 2 partitions we can collapse that to one partition using coalesce without bring any shuffle in the data.
words.repartition(10)
	-- Repartition allows us to repartition that data like we want but this is gonna cause a shuffle across the nodes in the process.Increasing the number of partition may cause parallelism while performing map and filter operations.
Repartition and sort with in partition 			
	-- This operations gives us ability to repartition as well as specify the ordering of each one of those output partitions.
CustomPartitioning:
	-- This ability is one of the reasons we will use RDD's. CustomPartitioners are not available for structure API's because they don't really have counter logical part
	-- They are lowlevel implementation details that can give a significant effect on weather our jobs run sucessfully.
	-- The cononical example to motivate customer partitoner for this operation is pagerank.where by which we seek to control the layout of the data on the cluster and avoid shuffles.
	-- In our shopping dataset this mean partitioning by customerID and in a simple way goal of custompartitoner is even distribution of dataover over the cluster.
	-- If we are gonna use customer partitioner we need to to drop down to RDD's and then perform custompartitioning and then switch back to RDD's. 
	-- To perform custom partitioning you need to implement our own class that extends partitioner.
	-- We need do this only we have enough knowledge in the process and all the details about the cluster ad process if we are just looking for partitioning on a value or set of values it worth doing it in dataframe API's
val df = spark.read.option("header", "true").option("inferSchema", "true").csv("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/retail-data/all/online-retail-dataset.csv")
	-- Here we are jsut reading file into a Dataframe.
val rdd = df.coalesce(10).rdd
	-- Here we are performing coalesce and changing into a RDD. Spark has 2 API's that we can leverage of in RDD API's HashPartitioner for descret values and RangePartitioner.
	-- These 2 work for discreate as well as countinous values 
import org.apache.spark.HashPartitioner
rdd.map(r => r(6)).take(5).foreach(println)
val keyedRDD = rdd.keyBy(row => row(6).asInstanceOf[Int].toDouble)
keyedRDD.partitionBy(new HashPartitioner(10)).take(10)
	-- Here we are usinh hashpartitioner. although this hash and range are usefull they are fairly rudmentary.At times we will need to perform lowlevel partitioning because we are working with large amount of data and large keyskew.
	-- Keyskew meand some keys have many many more values then other keys.
	-- We will need to break this keys as mush as possible to increase parallelism and also to avoid getting outofmemory exception.
	-- One instance might be we need to partition more keys if and only if key matches a certain format. For instance we might know that there are 2 customers in our dataset who crash our analysis and we need to break them up further than one customer ID.In fact there both are so skewed that they need to be operated seperately where as all others can be lumped into large groups.
	-- This a caricatured example but there is a chance that we see this king of thing in the data.
import org.apache.spark.Partitioner
class DomainPartitioner extends Partitioner {
def numPartitions = 3
def getPartition(key: Any): Int = {
val customerId = key.asInstanceOf[Double].toInt
if (customerId == 17850.0 || customerId == 12583.0) {
return 0
} else {
return new java.util.Random().nextInt(2) + 1
}
}
}
keyedRDD.partitionBy(new DomainPartitioner).map(_._1).glom().map(_.toSet.toSeq.length).take(5)				
	-- After we run this we see count of results in each partion.The second 2 number will vary because we are distrubuting them randomly.
CustomerSerialization:
	-- This is nothing but kyroserialisation. Any object that we hope to parllalize must be serialiazable.
class SomeClass extends Serializable {
var someValue = 0
def setSomeValue(i:Int) = {
someValue = i
this
}
}
sc.parallelize(1 to 10).map(num => new SomeClass().setSomeValue(num))
	-- the default serialisation will be quite slow.Spark can use kyro libraries to serialize the objects very quickly.Kyro is siginificantly faster and more compact than java serilization but doesn;y support all serializable types and askes to register the classes we will use in the program in advance for best performance.
	-- We can use kyro by initializing the job with sparlConf and setting the value spark.serializer to org.apache.spark.serializer.KryoSerializer
	-- this setting configure the serializer used for shuffling data between work nodes and serializing RDD's to disk.
	-- the only reason the kyro is not default because the custom registration requirement but it is recommened to use in any operations which have network intensive operations.
	-- Since Spark 2.0.0, we internally use Kryo serializer when shuffling RDDs with simple types, arrays of simple types, or string type.	
	-- Spark automatically includes Kryo serializers for the many commonly used core Scala classes covered in the AllScalaRegistrar from the Twitter chill library.
	-- To register your own custom classes with Kryo, use the registerKryoClasses method:
val conf = new SparkConf().setMaster(...).setAppName(...)
conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))
val sc = new SparkContext(conf)

val myCollection = "Spark The Definitive Guide : Big Data Processing Made Simple".split(" ")
	-- For example we have list of values or words.
val words = spark.sparkContext.parallelize(myCollection, 2)
	-- Same old same old
	-- and we want to sumplement the list of words with other information which we have which is many many giga or kilo bytes. This techincallya rightjoin when we speak in SQL terms.
val supplementalData = Map("Spark" -> 1000, "Definitive" -> 200,"Big" -> -300, "Simple" -> 100)			
	-- Here are we are taking some data and assigning it to a varaible.
	-- we can brodcast this structure across the spark and reference it by using supportBroadCast.This value is immutable and is lazily replicated across all nodes in the cluster when we trigger a action.
val suppBroadcast = spark.sparkContext.broadcast(supplementalData)
	-- We reference this varaiable by a value method which returns the exact value that we has earlier.This method is accesiable for serialized functions without having to serialize data .
	-- This saves us great deal of serializing data as spark can transfer data more efficiently around the cluster using broadcasts.
suppBroadcast.value
	-- now we could transform the RDD using this value. In this instance we will create a key-value pair ccroding to value we might have to map. if we lack value we simple replace it with 0.
words.map(word => (word, suppBroadcast.value.getOrElse(word, 0))).sortBy(wordPair => wordPair._2).collect()		
	-- We are using broacast value in a RDD.
Output:[('Big', -300),
('The', 0),
...
('Definitive', 200),
('Spark', 1000)]
Accumulators:
	-- Accumulators are a way of updating a value inside of variety of transformations and propagating that value to the driver node in an efficent or fault tolerance way.
	-- Accumulators provide a mutable varaible that a spark cluster can safely update on a per-row basis.We can use these for debugging purposes or to create lowlevel aggregation.
	-- Accumulators are varaiables that are added to only through an associative or commutative operation and can therefore be efficeiently supported in parallel.
	-- We can use them to implement counters or sums. spark natively supports accumulators of numeric type and programmers can add and support of new types.
	-- Accumulator's updates are performed inside of an action only spark guarntees that each tash update to the accumulator will be applied only once meaning restarted task will not update the accumulators.
	-- In tansformations we should be aware that each task update  can be applied more than once if tasks or job stages are reexcuted.
	-- Accumulators will not change the lazy evaluation of spark. If an accumulator is updated with in a operation on an RDD its value is update only once the RDD is computed.
	-- consequently accumulators updates are not guarnteed to be executed when made with in a lazy transformation like map.Acumulators can be both named and unnamed named will be displayed on SparkUI where as unnamed won't.
case class Flight(DEST_COUNTRY_NAME: String,ORIGIN_COUNTRY_NAME: String, count: BigInt)			
	-- Here we are defining a case calss.
val flights = spark.read.parquet("/data/flight-data/parquet/2010-summary.parquet").as[Flight]	
	-- loading flights data into a dataframe.
	-- Now we will create a accumulator that will count number of flights from and to china.Accumulators provides us to do this type of counting in programatic way.
import org.apache.spark.util.LongAccumulator
	-- Here we are importing a statemnet to implement the above functions.
val accUnnamed = new LongAccumulator
	-- Here we are assigning a new longaccummulator to a variable.
val acc = spark.sparkContext.register(accUnnamed)
	-- Here we are registering the accumulator and assigning it to a variable.
	-- The above is the unnamed accumulator.
val accChina = new LongAccumulator
	-- Same 
val accChina2 = spark.sparkContext.longAccumulator("China")
	-- This time we are naming our accumulator because it suits our usecase
spark.sparkContext.register(accChina, "China")
	-- then again we are registering the accumulator with name china.
	-- We specify the name of the accumulator in the string value that we pass into the function, or as thesecond parameter into the register function. Named accumulators will display in the Spark UI,whereas unnamed ones will not.						
def accChinaFunc(flight_row: Flight) = {
val destination = flight_row.DEST_COUNTRY_NAME
val origin = flight_row.ORIGIN_COUNTRY_NAME
if (destination == "China") {
accChina.add(flight_row.count.toLong)
}
if (origin == "China") {
accChina.add(flight_row.count.toLong)
}
}
	-- Here we are defining a function for accumulator.Now, let’s iterate over every row in our flights dataset via the foreach method.	
	-- The reason for this is because foreach is an action, and Spark can provide guarantees that perform only inside of actions.
	-- The foreach method will run once for each row in the input DataFrame (assuming that we did not filter it) and will run our function against each row, incrementing the accumulator accordingly:
flights.foreach(flight_row => accChinaFunc(flight_row))
	-- now the action will be performed and accumulator will be applied with the values.
CustomAccumulators:
import scala.collection.mutable.ArrayBuffer
import org.apache.spark.util.AccumulatorV2
val arr = ArrayBuffer[BigInt]()
class EvenAccumulator extends AccumulatorV2[BigInt, BigInt] {
private var num:BigInt = 0
def reset(): Unit = {
this.num = 0
}
def add(intValue: BigInt): Unit = {
if (intValue % 2 == 0) {
this.num += intValue
}
}
def merge(other: AccumulatorV2[BigInt,BigInt]): Unit = {
this.num += other.value
}
def value():BigInt = {
this.num
}
def copy(): AccumulatorV2[BigInt,BigInt] = {
new EvenAccumulator
}
def isZero():Boolean = {
this.num == 0
}
}
val acc = new EvenAccumulator
val newAcc = sc.register(acc, "evenAcc")
// in Scala
acc.value // 0
flights.foreach(flight)
val spark = SparkSession.builder().appName("Databricks Spark Example").config("spark.sql.warehouse.dir", "/user/hive/warehouse").getOrCreate()
df1 = spark.range(2, 10000000, 2)
df2 = spark.range(2, 10000000, 4)
step1 = df1.repartition(5)
step12 = df2.repartition(6)
step2 = step1.selectExpr("id * 5 as id")
step3 = step2.join(step12, ["id"])
step4 = step3.selectExpr("sum(id)")
step4.collect()
== Physical Plan ==
*HashAggregate(keys=[], functions=[sum(id#15L)])
+- Exchange SinglePartition
+- *HashAggregate(keys=[], functions=[partial_sum(id#15L)])
+- *Project [id#15L]
+- *SortMergeJoin [id#15L], [id#10L], Inner
:- *Sort [id#15L ASC NULLS FIRST], false, 0
: +- Exchange hashpartitioning(id#15L, 200)
: +- *Project [(id#7L * 5) AS id#15L]
: +- Exchange RoundRobinPartitioning(5)
: +- *Range (2, 10000000, step=2, splits=8)
+- *Sort [id#10L ASC NULLS FIRST], false, 0
+- Exchange hashpartitioning(id#10L, 200)
+- Exchange RoundRobinPartitioning(6)
+- *Range (2, 10000000, step=4, splits=8)
spark.conf.set("spark.sql.shuffle.partitions", 50)
== Physical Plan ==
*(7) HashAggregate(keys=[], functions=[sum(id#20L)])
+- Exchange SinglePartition
   +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#20L)])
      +- *(6) Project [id#20L]
         +- *(6) SortMergeJoin [id#20L], [id#16L], Inner
            :- *(3) Sort [id#20L ASC NULLS FIRST], false, 0
            :  +- Exchange hashpartitioning(id#20L, 50)
            :     +- *(2) Project [(id#14L * 5) AS id#20L]
            :        +- Exchange RoundRobinPartitioning(5)
            :           +- *(1) Range (2, 10000000, step=2, splits=16)
            +- *(5) Sort [id#16L ASC NULLS FIRST], false, 0
               +- Exchange hashpartitioning(id#16L, 50)
                  +- Exchange RoundRobinPartitioning(6)
                     +- *(4) Range (2, 10000000, step=4, splits=16)
    -- Defaulty the number of partition that should out put is 200 and we change that by doing spark.conf.set("spark.sql.shuffle.partitions", 50) and there is rule of thumb that the number if output partition should be always greater than the number of executors on the cluster.
    -- Stages and these stages are calculated according to the shuffles on the data.
Task:
	-- Is suppose we have dataset which have on single big partition of data then there would only be one task where as we have a data set which have 100 little partition then we will have 100 different task.
	-- A task is unit of compliation applied to unit of data. The more number of partitions more parallelism and and more the optimised the task can be.
	--                       












val connection = DriverManager.getConnection(jdbc:mysql://localhost:3306/SampleDatabas","root","light")
connection.isClosed()
																																																																						
jdbc:mysql://localhost:3306/SampleDatabas","root","light"))

val connection="jdbc:mysql://localhost/Acadgild"

val connection="jdbc:mysql://localhost:3306/SampleDatabas"
val mysql_props = new java.util.Properties
mysql_props.setProperty("user","root")
mysql_props.setProperty("password","light")


val dataframe_mysql = sqlcontext.read.format("jdbc").option("url", "jdbc:mysql://localhost:3306/SampleDatabas").option("driver", "com.mysql.jdbc.Driver").option("dbtable", "persons").option("user", "root").option("password", "light").load()

val flights = spark.read.parquet("/Users/kartikeyan/Desktop/Textbookie/Spark-The-Definitive-Guide-master/data/flight-data/parquet/2010-summary.parquet").as[Flight]

